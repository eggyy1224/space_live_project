# 星際小可愛 (Space Live Project) 自動化測試計畫書

## 簡介

「星際小可愛」專案是一個結合前端 TypeScript/React/Three.js 與後端 Python/AI 的即時互動虛擬網紅對話平台。近期專案進行了大規模重構，包括引入**多層次記憶系統**與**對話流程圖引擎**等核心改進。為確保重構後的系統穩定可靠，現制定一套自動化測試策略。此計畫書將說明如何針對前端、後端，以及前後端整合進行不同層級的測試（單元、元件、整合、端對端），並設計模擬使用者行為的測試腳本，以及在 CI/CD 中整合這些測試。目標是讓開發者能快速發現迴歸錯誤，確保每次修改不會破壞現有功能，同時為未來擴充提供信心。

## 專案架構與近期重構背景

**架構概覽：** 本專案分為**前端**和**後端 AI 核心**兩部分。前端使用 React + TypeScript 實作 UI，結合 Three.js（透過 React Three Fiber）呈現高品質 3D 虛擬角色。前端包含聊天介面、控制面板、音訊控制等元件，並透過單例服務（如 `WebSocketService`、`ChatService`、`AudioService`、`ModelService`）處理資料流：例如，錄音後上傳音訊，接收後端透過 WebSocket 推送的對話文字與動畫指令並在3D模型上執行。後端基於 FastAPI，提供 WebSocket 通道進行即時雙向通訊，以及 REST API 供音訊上傳、取得歷史記錄等操作。後端的**AI Orchestration**核心包括大型語言模型介接、對話管理，以及多層次記憶系統。對話管理部分使用 `DialogueGraph`（由 LangGraph 狀態機引擎實現）來控制對話流程和狀態。記憶系統 (`MemorySystem`) 則由多個子模組構成：短期記憶快取、對話長期記憶庫、角色人格記憶庫、摘要記憶庫等（基於向量資料庫 Chroma 實現持久化），以及查詢組建器、記憶檢索器、記憶格式化等，用於在生成回答時檢索並整合相關記憶。前後端透過 WebSocket 保持即時溝通：前端送出使用者文本（由 STT 模組轉換語音而得），後端處理後返回AI的文本回應和角色表情/動作指令，前端據此驅動3D模型的口型同步和表情變化。

**近期重構重點：** 為了提高系統的可維護性與擴展性，後端核心進行了重要重構：

- **記憶系統模組化：** 重構後的 `MemorySystem` 採用了協調者模式，將記憶劃分為**儲存層**、**檢索層**、**處理層**三部分。儲存層包括短期記憶 (`ShortTermMemoryStore`)、對話記憶、角色記憶、摘要記憶（後三者透過 Chroma 向量資料庫實現），確保不同類型記憶持久保存並易於檢索。檢索層包含 `QueryBuilder`、`MemoryRetriever` 等，用於根據使用者輸入動態生成查詢並取得最相關的記憶（例如採用最大邊際相關性(MMR)演算法避免檢索結果冗餘）。處理層則有 `InputFilter`（過濾無意義輸入）、`PersonaUpdater`（根據對話更新角色設定）、`ConversationSummarizer`（定期總結對話）等，提供更高階的記憶維護功能。這些改進引入了更多元件間的互動，需透過測試確保各部分協同正常。
- **對話流程圖引擎引入：** 先前的對話邏輯重構為基於 `DialogueGraph` 的流程圖實現。現在對話管理使用狀態機來明確定義各種對話狀態 (`DialogueState`) 及轉換條件。新增的節點負責對使用者輸入進行分類（例如檢查重複、分析情緒、判斷意圖類型），以選擇適當的提示模板和回應風格。`DialogueGraph` 提供了錯誤處理與重試機制，並可根據角色當前的情緒/狀態調整回答。這種結構化對話管理大幅提升了對話的魯棒性和可擴充性（未來可更容易地引入新節點或工具使用）。然而，這也意味著需要更細緻的測試來驗證每種路徑和狀態轉換是否正確，比如不同情境下是否選用了正確的提示模板、錯誤恢復機制是否生效等。
- **外部服務整合：** 後端現在整合了 Google 的語音服務（STT/TTS）以及 Google Gemini LLM（透過 LangChain 介接）。外部 API 的調用經過封裝 (`AIService` 作為適配層)，並保持向下相容。但為了在測試中避免對第三方服務的依賴與費用，需要對這些介面進行模擬。重構也引入了更多非核心檔案（例如 `.env` 環境變數設定、向量資料庫檔案等），測試環境需妥善配置隔離，避免與開發環境設定衝突。

綜上所述，本專案現代化的架構提供了豐富的功能，但也帶來相應複雜性。我們將採取分層次的自動化測試策略，針對各個模組和功能進行充分驗證，確保重構後各部分正確合作無誤。

## 前端測試策略

前端部分主要使用 **Jest** 測試框架搭配 **React Testing Library**（RTL）進行單元與元件測試，輔以必要的模擬工具來測試 WebSocket 互動和 3D 動畫邏輯。目標是覆蓋 UI 行為、狀態管理和服務通訊等方面。

- **單元測試 (Unit Tests)：**
    
    前端單元測試著重於純函式與服務邏輯。不涉及瀏覽器 DOM 的函式（例如字串處理、計算表情值的utility函式等）將撰寫獨立測試用例，驗證各種輸入輸出情形。對於服務單例（`ChatService`, `AudioService`, `WebSocketService`, `ModelService` 等）的關鍵方法也進行單元測試。例如：
    
    - 測試 `ChatService.sendMessage()` 方法：給定一段文字輸入時，是否正確使用 WebSocketService 發送訊息，以及在本地維護對話列表狀態。可以藉由替換或**模擬 WebSocket 物件**，監控 send 方法是否被呼叫以及呼叫的資料內容正確。
    - 測試 `AudioService` 的錄音與播放控制：模擬錄音開始與結束事件，確定內部狀態標誌（如「isRecording」）正確切換，並在結束錄音時調用上傳音訊的 API 客戶端方法。對播放部分，提供一個假 TTS 音訊 URL，驗證 `AudioService.playAudio()` 能正確播放音訊（在單元環境中可藉由檢查 audio 元素的狀態或使用 jest 對 Audio 對象的方法進行 spy）。
    - 測試 `ModelService` 的動畫更新邏輯：例如提供一組模擬的動畫指令（如讓角色點頭或改變表情的指令），確認 `ModelService` 能解析指令並更新對應的狀態值。如果 `ModelService` 對 Three.js 模型有直接操作，例如設定 morph target 的權重，我們可以將 Three.js 模型部分替換為**測試替身 (stub)**，只紀錄呼叫了哪些方法、參數為何，來驗證 `ModelService` 是否正確對模型進行操作。這確保角色動畫的指令處理邏輯在不依賴真實 WebGL 環境下被測試。
    - **WebSocket 通訊邏輯測試：** 前端大量使用 WebSocket 進行雙向通訊。我們會在單元測試階段模擬 WebSocket 連線：使用 Jest 提供的 fake timers 和 mock，可以暫時替換全域的 `WebSocket` 物件為一個可控的假物件。例如，可創建一個 `MockWebSocket` class，具有與瀏覽器 WebSocket 相同的介面 (onopen, onmessage, send 等)。在測試 `WebSocketService` 時，讓它使用 `MockWebSocket` 連線，模擬後端發送不同類型的訊息事件，檢查前端對這些事件的處理：比如當收到後端推送的對話文字時，是否正確呼叫 `ChatService` 更新訊息列表；當收到角色表情指令時，`ModelService` 是否被通知去更新模型表情。透過精細模擬 WebSocket，確保前端的即時通訊邏輯在各種訊息情況下都表現正確（例如連線中斷、自動重連、防抖頻機制等也可在此驗證）。
- **元件測試 (Component Tests)：**
    
    利用 React Testing Library 對前端 UI 元件進行渲染測試，模擬使用者操作並檢查元件輸出與狀態變化。重點元件包括聊天介面、控制面板、3D 模型顯示區等：
    
    - **聊天介面 (`ChatInterface` 元件) 測試：** 渲染聊天介面，傳入一個虛擬的 `ChatService` 實例做為 context，使其有初始對話記錄狀態。模擬使用者在文字輸入框中輸入訊息並按下送出按鈕（或按 Enter），驗證是否調用了 `ChatService.sendMessage()`（可用 jest.spyOn 監控），且 UI 上新的訊息氣泡正確出現在對話區域。接著，可以模擬 ChatService 接收到後端回應（例如我們直接呼叫 ChatService 的某個callback或利用前述 WebSocket模擬），觸發介面新增AI回應氣泡，確認顯示的文字內容與預期相符，並且具有正確的樣式（例如 AI 回應氣泡可能與使用者的不一樣顏色）。
    - **控制面板 (`ControlPanel` 元件) 測試：** 此元件可能包含控制3D場景和參數的多個子控制項（例如角色表情調整滑桿 `MorphTargetControls`、場景切換等等）。使用 RTL 渲染整個控制面板，模擬使用者點擊或拖動控制項：例如調整表情滑桿的值，期望 `ModelService.setMorphTargetValue()` 被呼叫，且參數對應正確的表情名稱和數值。也測試開關按鈕（如是否有切換背景音樂、鏡頭視角等控制）：模擬點擊後，檢查相應狀態改變或相應函式被觸發。若控制面板可以切換角色狀態，也可驗證對應的服務方法（如 `ModelService.changeScene()`）被呼叫。這些測試確保 UI 控制項和底層邏輯連結正確無誤。
    - **3D 模型顯示 (`ModelViewer` 元件) 測試：** 由於 Three.js 相關渲染在無頭測試環境中較難直接驗證，我們關注其 React wrapper 層的行為。例如，在 React 渲染該元件時，我們可以確定它會訂閱 `ModelService` 的狀態更新。可透過模擬 ModelService 發出事件（如角色表情改變事件）來測試 ModelViewer 是否正確接收並將更新應用到 Three.js 模型。實務上，可以在 ModelViewer 中將模型狀態存在 React state 或 context 中，我們便可在測試中檢查該 state 是否隨事件更新正確。若 ModelViewer 沒有容易存取的狀態，可考慮在測試環境中暴露 Three.js 模型對象，對其重要屬性做 spy（例如 morph target 值是否被改變）。需要的話，可使用 `jest-canvas-mock` 來模擬 和 WebGL 環境，以避免 Three.js 初始化拋錯。總之，此部分測試的目的是確保當有動畫/表情指令下達時，對應的3D呈現元件有執行更新流程（即使無法直接看見畫面，也至少能驗證呼叫流程）。
    - **邊界條件與錯誤UI處理：** 除了正常路徑，元件測試也應包含異常與邊界情況。例如：聊天室輸入框在空字串時送出按鈕應該是 disabled 狀態、錄音按鈕在未連線後端時是否呈現不可用、若 WebSocket 尚未連上時使用者送出訊息是否會排隊等待或顯示錯誤提示等。使用 RTL 檢查這些情況下的 UI 元素是否正確（如呈現警告訊息或按鈕灰化）。若專案對錯誤有特定處理（例如連線中斷時在介面上彈出提示），我們也要模擬觸發相應情境並驗證 UI 反應。
- **前端整合測試 (Frontend Integration Tests)：**
    
    當個別元件和服務都通過單元測試後，我們計畫進一步編寫前端層級的整合測試，把多個模組組合起來驗證整體行為。這類測試可使用 RTL 搭配較高階的渲染，甚至使用 **Jest DOM** 提供的自訂匹配器來輔助驗證 DOM 狀態：
    
    - **模擬完整對話流程：** 渲染整個 App 主元件（包含所有子元件和服務上下文提供者），但在測試環境下**攔截網路請求**。例如使用 `msw (Mock Service Worker)` 或 Jest 提供的 `fetch` 模擬，攔截前端對後端 REST API 的請求，直接返回預定義的結果。針對 WebSocket，可在 App 載入時用我們的 `MockWebSocket` 替換掉原生 WebSocket，使 App 以為已連上伺服器。接著，在測試中可以驅動整個對話：例如模擬使用者點擊錄音按鈕後，透過 MockWebSocket 的機制發送一條預先準備好的AI回應（包括文字和表情指令）。驗證整個 UI 從「使用者發話」到「AI 顯示回應並改變表情」這一流程是否通暢：期望聊天記錄出現使用者的發言內容，隨後 AI 的回覆文字氣泡也出現，且角色3D模型做出了對應的表情（這最後一點可透過檢查 ModelService 或模型 state 是否更新來間接驗證）。這種整合測試涵蓋了前端主要功能協同：音訊控制 -> 訊息傳遞 -> UI 展示 -> 模型更新。
    - **時序與性能相關測試：** 前端有些功能涉及節流/防抖（例如對頻繁的表情指令進行防抖，以免更新過於頻繁）。我們可以在整合測試中使用 Jest 的 fake timers 來模擬時間推進，測試防抖效果是否生效：例如快速連續地調用某表情更新函式多次，然後前進虛擬時間，檢查 ModelService 最終只應用最後一次更新而非每次都應用。另外，對於語音輸入流程，可測試錄音按鈕按下後持續一段時間再鬆開是否正確產生一筆音訊上傳請求，超過某時間長度是否自動停止錄音等（這需要在前端有相應邏輯時才測）。
    - **跨服務交互測試：** 例如 ChatService 同時與 WebSocketService 和 API Client 互動（送文字經 WS，送音訊經 REST）。我們可在測試中模擬 ChatService.receiveMessage 或相關 callback，驗證它對不同來源訊息的處理：如當 API 傳回 STT 文本結果時，ChatService 應該立即透過 WebSocketService 將文本發送給後端；又如當 WebSocket 收到 AI 回覆時，ChatService 應更新對話列表並調用 AudioService 播放 TTS 音訊（如果採用前端播放TTS的設計）。這類測試需要對 ChatService 初始化時注入的 service 模擬物件進行監控，確保 ChatService 在正確時機調用了正確的服務。通過這種交互測試，可以捕捉前端各服務之間接口對接是否協調一致。

前端測試將以**快速、單元為主，整合為輔**的原則實施：數量上單元/元件測試案例會佔多數，用於精細覆蓋各函式和元件邏輯；再挑選幾項關鍵用例做整合測試，確保主要模組互動正常。所有前端測試可透過 `npm test` 或 `yarn test` 自動執行，並使用 coverage 工具統計覆蓋率，以確認 UI 和服務邏輯的關鍵部分皆有測試到。

## 後端測試策略

後端將採用 **PyTest** 作為測試框架，編寫單元與整合測試來覆蓋對話引擎、記憶系統和 API 介面等模組。由於後端涉及非同步處理、外部 API 調用及資料庫操作，我們會善用 PyTest 的擴充（如 `pytest-asyncio` 支持 async 測試、fixtures 管理資源等）以及模擬物件來隔離外部依賴。重點如下：

- **單元測試 (Unit Tests)：**
    
    後端單元測試聚焦於**純邏輯函式和演算法**，不涉及外部服務或IO。例如：
    
    - **對話流程邏輯：** 測試 `DialogueGraph` 中的各個節點處理函式。例如，如果有節點負責檢測重複輸入，我們可以直接調用該函式，輸入一些過去訊息列表和一個新輸入，驗證返回結果（或狀態轉換）是否標記為重複。對於輸入分類節點（如情緒分析），我們可為其設計多種輸入文本（正向、負向、疑問句等）並驗證其分類結果符合預期的類別。如果 `DialogueGraph` 的某些部分依賴 LLM 回應，我們會在單元測試中**模擬 LLM**：例如猴擒(monkeypatch)調用 OpenAI/Google API 的函式，使其直接返回我們預設的回覆文字，從而測試後續的後處理邏輯。藉由這種方法，可以驗證 `DialogueGraph` 的**動態提示模板選擇**：給定不同對話上下文，對話引擎是否選擇了正確的 Prompt 模板（正常對話 vs 澄清 vs 更正錯誤等）；**回應後處理**：我們構造一個包含多餘標點或 Emoji 的假輸出，測試 `DialogueGraph` 的後處理函式是否清除了不需要的部分並保留應有內容。對於錯誤處理邏輯，我們可以模擬 LLM 連續兩次拋出例外，確認 `DialogueGraph` 是否依設計在第二次失敗後走備用分支/返回預設回應。這些單元測試確保對話引擎中每個細小的決策點在各種情況下都表現正確。
    - **記憶系統模組：** 單元測試會覆蓋記憶系統的組件演算法。例如：
        - `QueryBuilder`：輸入一段使用者提問和部分歷史對話片段，確認產生的查詢字串包含關鍵字並加上必要的上下文提示（例如可能在問題前後附加上一些系統提示語）。我們可以提供不同風格的輸入，檢查 QueryBuilder 是否都能產生有效查詢。
        - `MemoryFormatter`：給定一些原始記憶項，例如對話記錄物件，MemoryFormatter 應將之格式化成人類可讀的摘要文字。我們測試各種記憶（短期對話 vs 人格設定 vs 對話摘要）是否格式化後區分明確，比如對話記憶是否加上時間戳或發言者標識等。如果格式超出預期長度，也要看 Formatter 是否有截斷等處理。
        - `InputFilter`：輸入各種句子測試其過濾效果。比如空白或只有重複字元的輸入應被標記為無效、包含罵人字眼的輸入若有過濾髒話功能則應被攔截等等。我們不需真正連到服務器，只要直接調用此函式/類的方法即可驗證邏輯。
        - 情緒分析/狀態更新：如果重構中加入了 `EmotionAnalyzer`（情緒分析）或在 PersonaUpdater 中有分析對話內容決定角色心情的邏輯，我們會對此撰寫測試。提供幾組帶明顯情緒的對話內容給 PersonaUpdater，看它是否更新角色情緒狀態屬性到正確的值（例如悲傷、開心等枚舉值）。這確保角色狀態演算法運作符合預期。
    - **其他輔助函式：** 包括對話摘要生成器 (`ConversationSummarizer`) 中可能有分段摘要或合併摘要的函式。我們可模擬一些對話內容，直接呼叫摘要函式並比對輸出是否包含關鍵資訊（由於摘要本身由 LLM 生成且具隨機性，這裡我們會 monkeypatch 掉真正的摘要API調用，讓它返回預設段落，以驗證我們對該段落的儲存流程是否正確）。另外，如果有自定義的工具函式（例如格式化時間、從資料結構中移除最早對話以維持短期記憶大小上限等），都應有對應的單元測試防止日後改動引入邏輯錯誤。
- **整合測試 (Integration Tests)：**
    
    後端整合測試主要檢驗各模組之間的協作，以及對外介面的正確性。我們將使用 PyTest 的 fixture 來搭建**小型測試環境**（例如啟動一個測試用的 FastAPI 應用或 Memory 資料庫），然後運行一系列互動以驗證系統行為。
    
    - **記憶儲存與檢索整合測試：** 構建一個實際的 `MemorySystem` 實例（或甚至啟動整個 FastAPI app，以取得依賴注入的 MemorySystem）。使用一個**測試專用的向量資料庫**位置：透過設定環境變數或 fixture，在執行測試時將 Chroma DB 路徑指向臨時目錄，確保不影響真實資料。測試內容包括：
        1. **新增記憶條目**：模擬多輪對話，透過 MemorySystem 將對話內容新增到短期記憶和長期記憶庫。我們可以直接呼叫 MemorySystem 提供的介面方法（例如 `MemorySystem.store_conversation(user_input, ai_response)`），或透過 DialogueGraph 執行一輪對話讓其內部調 MemorySystem（在測試中使用假 LLM 讓對話快速完成）。新增後，直接查詢底層存儲（例如調用 `ChromaMemoryStore.get_all_memories()` 或使用 chromadb 的API）確認新記憶確實被保存。也可檢查短期記憶緩存列表的長度是否達到預期。
        2. **檢索驗證**：向 MemorySystem 請求與某關鍵字相關的記憶（例如呼叫 `MemorySystem.retrieve_relevant(query)`）。我們預先在存儲中放入幾筆含特定關鍵詞的對話記錄，然後以該關鍵詞做查詢，檢查 MemoryRetriever 返回的結果是否包含那些相關記憶，且順序可能已根據相關度排好。如果 MemoryRetriever 實現了 MMR 去重，我們也提供一些相似度很高的記憶項，測試返回結果沒有重複資訊（例如不會同時取回兩段幾乎相同的對話）。另外，檢查 MemoryFormatter 是否在此流程中發揮作用：結果文字是否格式清晰（例如加了「AI 說：...」前綴等）。這部分測試確保 MemorySystem 的各子模組在一起運作時能成功完成“存-取”循環。
        3. **記憶更新處理**：測試 PersonaUpdater 和 ConversationSummarizer 的整合。比如模擬一段包含角色新資訊的對話，之後調用 PersonaUpdater 將資訊寫入角色記憶庫，再檢索角色記憶庫確認新資訊在其中（例如角色的興趣愛好已更新）。對摘要，連續輸入多輪對話後，調用 ConversationSummarizer（可能 MemorySystem 自動調用或我們手動調用）生成摘要，檢查摘要存儲中是否新增條目，以及摘要內容是否符合最近幾輪對話的要點（此處同樣可用假 LLM 輸出）。
    - **API 介面測試：** 使用 **FastAPI 提供的 TestClient** 對後端各API進行測試，包括 WebSocket 和 REST endpoint：
        - **WebSocket 對話介面：** 啟動測試客戶端，使用 `client.websocket_connect("/ws")` 連接到後端的 WebSocket（假設路徑為 /ws 或專案中定義的socket路由）。連線建立後，模擬發送一條使用者訊息。由於真正的對話需要呼叫 LLM，我們在這個測試中**替換 AIService 的 LLM 呼叫為假實現**：可以在建立 FastAPI 應用時，注入一個假的 AIService，內部邏輯是接收到任何輸入都回傳預設的回覆，例如「Hello, this is a test response.」以及一個隨機表情指令。這樣，我們透過 WebSocket 發送測試訊息，比如：`{"text": "Hello"}`，預期從 WebSocket 收到的返回訊息包含 AI 的回覆文本 "Hello, this is a test response." 以及一條動畫指令（格式可能為 JSON 包含表情名稱和強度）。我們驗證接收的消息JSON結構正確無誤，字段齊全，類型符合定義。如果專案協議中約定一次對話可能分多條消息（例如先發回覆文字，後發動畫指令），我們也需要確保在合理時間內收到了所有該收的消息。利用 pytest-asyncio，可以等待一定時間收集所有從 websocket 傳來的消息，再逐一檢查。這確保 WebSocket 通道在典型對話場景下工作正常。此外，我們也會測試**邊界情況**：例如發送空訊息或非常長的訊息，後端是否返回錯誤碼或錯誤消息；若在對話過程中記憶系統沒有找到相關記憶，回傳內容是否依然合理（這可能需要在假AIService中模擬不同情況）。也可以模擬在WebSocket尚未完全連線時發送資料，確保伺服器正確處理或關閉連線。
        - **音訊上傳與語音識別 API：** 後端預期有一個 REST API 接口讓前端上傳錄製的音訊並獲取STT結果（文字）。使用 TestClient 對該端點發送一個測試音訊檔案。例如我們準備一小段WAV檔（內容可讀出「Hello」）作為測試資源，透過 `client.post("/stt", files={"file": audio_data})` 發送。由於不希望測試真的調用Google API，我們可以 monkeypatch 掉後端處理該請求的函式中與Google STT交互的部分，讓它直接返回預先定義的文字（例如 "Hello"）。檢查API回應的狀態碼應為200，且JSON結構包含預期的文字結果欄位（如 `{"text": "Hello"}`). 也可測試上傳非音訊檔或非常短的無效音訊時，API是否回傳適當錯誤碼或錯誤訊息。
        - **文本到語音 API (若有)：** 如果後端提供TTS端點，測試發送一段文字，後端應回傳合成的語音（二進制串或URL）。我們同樣會替換真正的TTS服務呼叫，以免浪費資源。例如令函式直接讀取一個現成的音訊檔作為假回應。測試重點在狀態碼=200且回應格式正確（比如Content-Type是audio/wav或 JSON內含音訊檔URL等，看實際實現）。此外，可測試請求超出長度限制時是否給出錯誤。
        - **其他後端路由：** 若有提供如 `/history`（取得過往對話記錄）、`/persona`（取得或更新AI人格設定）等API，也需測試：請求不帶授權是否被拒、正確參數回傳正確資料、錯誤參數回傳400等等。這部分依專案具體接口而定。透過 TestClient 可以方便地模擬各種HTTP動詞和參數組合，並斷言回應。
    - **對話系統端對端整合測試：** 在單元測試驗證細節、上述各子系統整合測試驗證子系統協作後，我們還計畫在後端實施一個高層級的集成測試，用更貼近實際使用的方式跑完整對話流程。這會結合 DialogueGraph、MemorySystem 以及假 LLM，模擬一次完整的對話交互：
        
        利用一個**假 LLM**（例如繼承某 LLM class，覆寫其 `generate_response(prompt)` 方法，使其對任何 prompt 都回傳固定句子如 "Roger that."），將此假 LLM 注入 AIService。然後：
        
        1. **初始化狀態：** 清空/初始化測試用的記憶資料庫。啟動 FastAPI 應用（在測試模式下，例如設置 `TEST_MODE=True` 讓應用使用假 LLM和假Google API）。
        2. **模擬對話：** 用 TestClient 的 websocket 連線，上送一連串模擬使用者訊息。例如第一句「Hi, how are you?」，預期AI回覆固定"Roger that."（因我們假LLM如此設定）。第二句再送「Tell me about your memories.」，AI仍回"Roger that."。我們檢查每次收到的回覆是否都是"Roger that."（確認對話系統沒有邏輯錯誤地傳遞 prompt 給 LLM，以及返回處理正常），**並且**在第二次對話時，MemorySystem 應該已經有了第一輪對話的記錄可供檢索。為了驗證記憶檢索是否被使用，我們可以在假 LLM 中稍微聰明一點：讓它將 prompt 輸入的一部分反映到回答裡。比如假 LLM 被賦予功能：如果 prompt 中有特殊標記 "[MEM]"（代表檢索到的記憶內容）就回覆 "I remember: {記憶內容}"。然後我們可以在 MemoryRetriever 中設計使其一定加入一段測試記憶到 prompt（比如每次都加入 "TestMemory"），如此我們期望AI回覆包含 "I remember: TestMemory"。在收到實際回覆時驗證之。如果回覆缺少該內容，說明 MemoryRetriever 或 prompt 構建沒生效，測試因此能捕捉這類錯誤。這種方法雖然人工，但能驗證對話流程中記憶整合是否真的發生。
        3. **檢查狀態變化：** 對話結束後，調用後端提供的接口或直接存取應用內部的 MemorySystem，確認短期記憶列表包含兩輪對話，長期記憶向量庫內文檔數量增加了2條（如果每輪對話分別存一筆embedding）。若摘要條件達成（例如對話超過N輪），也可檢查摘要存儲中是否新增一筆。這些驗證確保對話流程不只輸出對話，**也**正確更新了內部狀態。
        4. **異常路徑測試：** 在集成測試中也模擬一些異常情況，如當 MemoryRetriever 找不到相關記憶時（可以讓它返回空），AIService 是否仍能回傳回應而不崩潰；或者讓假 LLM 第一次調用拋出例外，以測試 DialogueGraph 的重試機制是否進行了第二次調用並最終成功。這些都可以透過調整假LLM行為來實現並驗證結果。

透過上述多層次的後端測試，我們將確保：每個核心模組的內部邏輯正確，模組之間的互動符合預期，對外提供的服務介面穩定可靠且對各種輸入容錯處理妥當。所有 PyTest 測試可配置在 CI 中執行，並透過 pytest-cov 收集覆蓋率報告，以評估後端關鍵路徑（對話狀態轉換、記憶讀寫、API響應）的覆蓋程度。

## 前後端整合測試 (端到端功能測試)

前後端整合測試旨在不經由模擬或假物件，而是在**接近真實運行**的情況下驗證整個系統的資料流。這類測試通常屬於端對端 (E2E) 測試範疇，但我們在程式層面編寫，使其可自動執行。目標是從**前端動作**開始，經由網路傳輸，到**後端處理**，再回到前端呈現，涵蓋語音處理、對話、記憶更新全流程。

- **語音輸入到回應流程測試：** 我們將模擬實際使用者使用系統的一次完整交互。例如設計場景：「使用者按下麥克風錄音並說出一句話，稍後AI做出回答並改變表情。」實現步驟：
    1. **啟動前後端測試伺服**：使用 `uvicorn` 在本地啟動後端（測試模式，禁用真實外部API）。前端可以使用開發伺服器或預先 build 後以靜態檔伺服。在測試代碼中，我們可以透過 `subprocess` 啟動這兩者，再等待幾秒確認服務就緒。
    2. **語音輸入模擬**：編寫一個腳本或使用測試框架的HTTP客戶端，向前端的音訊上傳接口發送一段預錄製的測試音訊（例如 "你好嗎" 的語音檔）。由於我們在測試模式可讓後端繞過真實STT，直接返回固定文字，例如「你好嗎」。前端收到該STT結果後會經由 WebSocket 將文字發給後端。我們可以在後端記錄 log（或透過 WebSocket 截取）確認該文字訊息確實到達 DialogueGraph。
    3. **對話處理與回傳**：後端處理該文字（流程包含Memory檢索、LLM生成等，但在測試模式LLM會給定固定回應如 "我很好，謝謝關心。"）。後端透過 WebSocket 將回應文字和一條隨機表情指令發送給前端。我們在前端側需要捕捉這個 WebSocket 訊息並驗證：聊天UI收到 AI 回覆 "我很好..." 並顯示，且 ModelService 收到表情指令後更新了角色表情。例如，若指令是讓角色微笑，我們期望 ModelService 的內部狀態 `currentEmotion` 變為 `smile`，或者控制3D模型的某個 `morphTarget` 值提高。我們可以在前端程式碼中臨時增設鉤子來將這些狀態輸出到測試控制台，或者更直接地，**在整合測試中使用瀏覽器自動化（如下一節將述的 Cypress）來觀察UI變化**。
    4. **記憶更新驗證**：上述對話完成後，在後端查詢記憶系統：短期記憶應新增了一條「使用者:你好嗎 / AI:我很好…」的記錄；長期記憶庫也應新增對應embedding。我們可以直接對後端發一個查詢請求（如果提供了debug API，比如`/debug/memory`返回記憶列表）或透過後端日誌確認。另外，若會觸發摘要（假設一輪對話不會，一般多輪才會），可在這類測試中執行多輪交互後再檢查摘要生成。由於E2E測試著重驗證主要路徑，我們會優先確認「資料有進DB」即可，不深究內容正確性（內容在前述模組測試中已驗證）。
    5. **多輪對話情境：** 除了單輪問答，我們也可串聯多輪。例如連續三次發話，觀察AI每次回應是否合理且沒有遺漏上下文。尤其最後一次可問一個需要記憶的問題，比如：「剛才我問你的第一個問題是什麼？」——預期AI應利用MemorySystem取出最初問題並回答。我們驗證AI的回答是否包含那個問題內容。如果成功，代表記憶檢索在整體流程中運作正常。如果這點較難以斷言精確（因為需要AI語言理解），至少我們確認第三輪提問AI沒有答非所問或報錯即可。
    6. **資源清理與隔離**：每次整合測試應獨立進行，確保前一案例的資料不影響下一個。因此在測試之間我們重置後端（重啟應用或重置Memory DB文件）。PyTest fixtures可用`autouse=True`的fixture在每個test前後清除 `/tmp/test_chroma_db` 資料夾等，保證記憶資料獨立。另外前端如有全域狀態也清除或重新載入頁面。

這類前後端整合測試通常耗時較長，不會對每個細節分支都測試，而是針對**關鍵用例**做端到端驗收。例如**正常語音對話流程**、**連續多輪對話**、**斷線重連**（可在測試中強制重啟後端或斷開 WebSocket 看前端是否能自動重連並繼續會話）等。這些測試可以使用腳本自動執行，但更直觀的方法是使用下節的瀏覽器自動化腳本來完成。同時，在CI中我們可能將此類測試標記為E2E類型，與單元測試分開執行，以便在本地調試時可以選擇性跳過。整合測試成功通過即代表**整個系統**在典型使用場景下表現符合預期。

## 模擬瀏覽器端使用者行為測試 (E2E UI 測試)

為了以使用者視角驗證系統，我們將使用瀏覽器自動化測試工具（如 **Cypress** 或 **Playwright**）編寫端對端測試腳本。這些測試在真實瀏覽器環境中執行，模擬使用者與UI互動，並觀察UI反饋以及前端與後端協作的結果。重點涵蓋：頁面載入、對話流程、角色表情反應，以及任何使用者能操作的界面元素。

**環境設定：** E2E 測試需在有前後端服務的情況下運行。通常我們會在測試前**佈署應用**：例如使用 docker-compose 啟動前端（可能以production build提供靜態頁面）和後端（Uvicorn啟動API）容器，或者在CI中分步啟動。為簡化，我們也可在測試開始時由 Cypress/Playwright 啟動一個本地開發伺服器。在測試環境中，我們會使用與開發相同的設定，但可以載入測試專用的環境變數（如使用假API金鑰、不啟用分析工具等）。尤其**AI相關的外部依賴**要處理：我們可能在啟動後端時設置環境變數使其改用假 LLM 回覆，以避免每次E2E測試都真的請求外部服務。

以下是計畫的 E2E 測試腳本場景：

- **場景1：頁面載入與基本UI呈現**
    
    進入應用的主頁（例如 `http://localhost:3000`），檢查頁面元素是否正確載入：包括3D模型容器是否存在、聊天區是否渲染、控制面板和錄音按鈕是否可見。用戶看不到的錯誤不可出現（Cypress會自動捕捉console errors，可將test標記失敗）。我們會斷言一些基本元素：例如`.chat-container`應存在且空（尚無對話）、`.model-canvas`元素加載完成（Three.js初始化完成的指標也許是canvas存在且有適當尺寸）、控制面板的若干按鈕（如表情滑桿、重置姿勢按鈕等）都應呈現。這確保應用基本界面在初始載入沒有崩潰並正常顯示。
    
- **場景2：文字對話流程**
    
    如果應用允許使用者直接輸入文字（有聊天輸入框），我們將使用它進行對話（若只有語音，則模擬語音，下一個場景涵蓋）。在輸入框輸入「你好」並按Enter送出。預期：畫面立即顯示使用者訊息氣泡「你好」，並且後端在短時間內回傳AI回覆。測試腳本將**等待**聊天區出現新的AI回覆氣泡（例如包含預期的AI名字或樣式）。出現後，檢查文字內容是否合理（如果我們配置後端假LLM返回固定回覆「你好，人類」之類，那我們就斷言這精確字串；若未固定，至少斷言氣泡內容非空且可能包含問候語）。然後檢查3D模型是否有反應：例如在AI回覆出現時，預期角色嘴型會張合同步發音以及表情變化。我們可以透過**截圖比對**或DOM狀態檢查的方法驗證表情變化：具體而言，模型本身在Canvas中渲染，我們無法直接讀取它的表情值，但**控制面板**上表情滑桿可能會隨著AI表情變化更新（如果有此設計）。假設角色笑時，「微笑」滑桿自動移動到高值，那我們可在AI回覆後查找該滑桿的位置值是否增大。若前端未將表情暴露在UI元素上，則退而求其次：我們可以在Cypress中截圖AI回覆前後的canvas，儘管像素比較不易精確，但若變化明顯（例如從中性臉到笑臉），可以比較某幾個關鍵像素顏色或快照進行差異比較。這種圖像比對能發現模型有無改變。若比對複雜，我們記錄此步需人工確認，但至少能保證**事件有觸發**（從前端log或Network面板看到有表情指令消息）。
    此場景也包括**驗證連續對話**：在第一次回覆後，再輸入第二句，例如「現在時間幾點？」，檢查新氣泡追加，AI回應出現（可預期假LLM回「抱歉我不知道時間」）。確認對話區按順序累積消息沒有亂序。最後可以測試清除對話（如果有清除按鈕）：點擊清除後，UI應移除所有消息氣泡。
    
- **場景3：語音對話流程**
    
    模擬使用者點擊錄音按鈕說話。我們可以利用 Cypress 提供的 `.trigger()` 事件功能，模擬點擊錄音按鈕後，透過前端暴露的MediaRecorder去注入音訊資料。然而，直接控制麥克風輸入在自動化中較難，因此我們採用**網路攔截法**：當使用者按下錄音按鈕，應用會開始錄音並最終透過 REST API 上傳音檔。我們在 Cypress 中使用 `cy.intercept()` 監聽這個上傳請求，在錄音按鈕按下後，等待拋出請求，然後用預製的回應取代：返回固定的 JSON，例如 `{ text: "天氣真好" }`，模擬 STT 解碼結果。如此一來，前端會收到「天氣真好」這個文本結果，進而在 WebSocket 通道送出。接著我們**不攔截 WebSocket**（讓它連真實後端或測試後端），後端（假LLM）會對"天氣真好"回覆一句話如"是啊，今天天氣很棒！"。我們再像文字場景一樣驗證AI回覆氣泡出現，內容正確。錄音按鈕本身也要測試：按下後應變成正在錄音狀態（可能按鈕顏色改變或出現倒數計時等UI）。我們斷言錄音圖示有變化（例如class包含 `recording`）。在我們用 intercept 模擬回傳結果時，其實代表錄音完成，前端應自動停止錄音並恢復按鈕狀態。我們檢查錄音按鈕是否回復可點擊狀態且取消`recording`樣式。這確保整個語音輸入UI循環運作正常。
    
    此外，我們也可以在這場景測試**錯誤處理**：例如攔截 STT 接口返回500錯誤，前端應在UI上給出提示（如「語音辨識失敗」）。我們讓按鈕按下->拋錯->檢查畫面是否出現錯誤訊息對話框或 toast。
    
- **場景4：控制面板與其他互動**
    
    測試使用者對控制面板各項功能的操作：
    
    - **表情手動調整：** 使用者拖動表情滑桿（如「生氣」表情值）。在 Cypress 中，可以選擇滑桿元素，使用 `.invoke("val", 0.8).trigger("change")` 模擬將值調到80%。預期3D模型會即時反映：例如眉毛下壓表情明顯。我們同樣可以通過截圖或檢查模型state變量（若控制面板有同步顯示值）來驗證模型確實改變。如果難以自動檢查，至少確保沒有錯誤，並假設Three.js自身功能正常。這步更多是確定滑桿事件有正確傳到 ModelService（其已在前端單元測試中驗證），E2E只需確認UI上滑桿位置和文本標示更新無誤。
    - **場景切換/其他按鈕：** 若控制面板有切換背景或角色服裝等按鈕，我們點擊後檢查畫面元素變化。例如切換場景可能改變場景光線或環境貼圖，這可以透過檢查canvas上一些屬性或body的某個class變化來驗證（具體視實現，例如可能在切換時整個canvas會重載，我們可assert新的canvas元素出現）。如果有重置按鈕，點擊後應將模型姿勢和表情歸中性，驗證表情滑桿回到中間位置等。
    - **音量/麥克風狀態：** 測試靜音按鈕（如果有）。點擊後或前後，確認對應圖示切換，且**不影響**對話流程（例如AI回覆仍產生但前端不播放聲音，此在自動化中難檢查聲音，可檢查Audio元素的 muted 屬性是否為true）。
    - **重新連線：** 模擬網路斷開再連接。可在 Cypress 裡暫停後端服務（或更激進：直接關閉 WebSocket 連線）。具體方法：Cypress無法直接控制後端，但可執行 `ws.close()` 通過瀏覽器關閉socket。前端應偵測到關閉，通常會嘗試重連或在UI上顯示「連線中斷」。我們檢查是否有這提示。數秒後再讓前端重連（如果有自動重連，則等待；若需人工重連按鈕，則點擊）。然後再發送一條訊息，確定依然得到回覆。這驗證應用的健壯性。

上述E2E測試用例著重**核心使用者路徑**：正常聊天、語音輸入、UI控制。每個測試均會在最後對**預期結果**進行明確斷言，包括UI文字、元素狀態、前端發出的請求等，以定位任何偏差。Cypress 等工具允許錄製測試過程，如果測試失敗，我們可回放了解發生了什麼。未來隨著新功能加入（例如多用戶支援或通知功能），也應在此新增對應的E2E腳本。

## CI/CD 中的自動化測試整合

為了在持續整合/部署流程中保證代碼品質，我們將把上述各層級測試納入 **GitHub Actions** 工作流程。在每次提交(push)或拉取請求(PR)時，自動執行測試套件，快速回饋問題。CI Pipeline 將分階段執行測試，以提高效率和定位問題：

1. **靜態分析 (Lint)**
    
    首先執行前後端的程式碼靜態檢查。利用 **ESLint** 檢查前端 TypeScript/JavaScript 代碼風格與潛在錯誤，**PyLint/Flake8** 檢查後端 Python 代碼。一旦發現lint錯誤（如未使用的變數、不符合規範的代碼），CI 即標記失敗，阻止後續步驟。這有助於維持整潔一致的代碼風格，也能提早發現低級錯誤。
    
    *實作細節：* 在 GitHub Actions workflow 中配置兩個job或步驟：例如 `frontend-lint` 使用 Node 最新 LTS 建立環境，執行 `npm ci && npm run lint`；`backend-lint` 使用 Python3.10 圖像，執行 `pip install -r requirements.txt && pylint backend/`。可以串行或並行執行，視需求決定。產出物如lint報告可上傳Artifacts方便查看。
    
2. **單元測試**
    
    接著執行前後端單元測試。這步將啟動 Node 環境跑 Jest 測試，以及 Python 環境跑 PyTest（標記為單元的部分）。要求所有測試通過且覆蓋率達到設定標準（如覆蓋率低於閾值時可讓CI失敗以強制補齊測試）。
    
    *實作細節：* `frontend-test` 步驟中，執行 `npm run test -- --ci --coverage`。`--ci` 模式下 Jest 遇到一次fail即返回錯誤碼，並生成 coverage summary。後端類似，執行 `pytest --maxfail=1 --disable-warnings -q` 提升速度，可加上 `--cov=prototype/backend` 收集覆蓋率。覆蓋結果可以藉由 Actions Upload 或整合 Codecov 等服務上傳。
    
3. **後端整合測試**
    
    單元測試後，再執行需要多模組互動的後端整合測試（包含記憶系統、DialogueGraph 和 API 的測試）。這些測試通常也用 PyTest，但可能標記為`integration`。我們可以設置 PyTest mark，讓CI在這一步用 `pytest -m integration` 執行，或區分成不同tox環境。這確保即使某些整合測試較耗時，也不拖慢前面快速單元測試的反饋。
    
    *實作細節：* 建立一個`integration-test` job，重用前面安裝好依賴的環境，執行 PyTest 並指定 marker。注意要提供測試用的依賴（如啟動一個測試用的 chroma vector DB server 或確保chromadb套件已安裝）。如果整合測試需要外部服務（如需要真的調用Google API），我們應在CI環境提供假的憑證並啟用測試模式來避免外調，以防CI阻塞在無法連網的請求上。
    
4. **端對端 (E2E) 測試**
    
    最後執行瀏覽器端的E2E測試（Cypress/Playwright）。這部分耗時相對較長，且需要完整的應用運行環境。流程為：**建置並啟動應用 -> 執行測試 -> 關閉應用**。
    
    - **建置應用：** 前端執行 `npm run build` 產出靜態資源，後端如使用uvicorn則不需要build但可以直接用目前代碼運行。確保在CI中生成前後端可執行的產物。
    - **啟動服務：** 常用的方法是在 workflow 中使用服務（services）配置，例如使用一個資料庫服務容器。如果需要，我們可定義一個包含chromadb的服務容器。對於應用本身，我們可在步驟腳本中啟動：例如 `uvicorn main:app --host 0.0.0.0 --port 8000 &` 在背景跑後端；`npx serve -s build -l 3000 &` 提供前端靜態頁面。然後等待幾秒讓服務就緒（或輪詢端口開放）。
    - **執行測試：** 若採用 Cypress，需安裝 Cypress CLI（可提前在前端依賴安裝時已裝好）。執行 `npx cypress run --browser chrome --headless` 跑所有E2E測試規範。Cypress 會接管Chrome進行腳本操作，最後輸出結果。對於 Playwright，相似地執行 `npx playwright test --reporter=line` 等。
    - **關閉服務：** 測試結束後，需要關閉先前啟動的前後端服務。可在 workflow 中透過`kill`命令結束進程，或者更優雅地，使用 **GitHub Actions的 job container**：例如用 docker-compose 在容器中跑服務，測試完停止容器。因為這部分屬於CI腳本編排，計畫書中點出需求即可。
    *實作細節：* 建立一個`e2e-test` job，此 job 需同時具備 Node（跑Cypress）和 Python（跑應用）的環境。一種方法是使用 Node 的官方 actions 中指定同時安裝 Python，例如:
    
    ```yaml
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with: { node-version: 18 }
      - uses: actions/setup-python@v4
        with: { python-version: "3.10" }
      - run: pip install -r prototype/backend/requirements.txt
      - run: npm ci && npm run build
      - run: nohup uvicorn prototype.backend.main:app --host 0.0.0.0 --port 8000 &
      - run: nohup npx serve -s prototype/frontend/build -l 3000 &
      - run: npx wait-on http://localhost:3000 && npx wait-on http://localhost:8000
      - run: npx cypress run --headless
    
    ```
    
    如上，使用 `wait-on` 等工具等待服務就緒再跑測試。測試完成後，Actions 會自動結束子進程。為了在失敗時也收集資訊，我們會設置 Cypress 輸出視訊/截圖到Artifacts，便於開發者調試。
    
5. **併發與流程控制：**
    
    我們考慮將 lint 和單元測試並行執行，以加快速度；而整合測試和E2E因資源較重，可串行在單一runner執行，或使用需要時才運行（例如每日構建或在主分支上執行E2E，以節省每次PR跑E2E的時間）。初期為求保險，每次PR都跑完整流程，待一段時間穩定後，可根據需要調整頻率。
    
6. **持續部署 (若適用)：**
    
    雖然重點在CI測試，但若有CD流程（如部署到測試伺服器），可在上述測試全部通過後才進行。也就是Workflow最後可以加入部署步驟（例如推送Docker映像或通知雲服務部署）。如此確保只有通過所有測試的版本才被部署。
    

**通知與報告：** 我們會將 GitHub Actions 的狀態徽章加入專案README，並設定在CI失敗時發送通知（GitHub自帶PR狀態以及email提示）。覆蓋率報告可上傳到像 Codecov 並在PR中註明，本專案近萬行代碼經測試後期望達到較高覆蓋率（例如後端80%以上，前端70%以上起步並逐步提升）。另外，可配置 **Slack/GitHub通知**：當主分支測試失敗時提醒團隊立即修復，保持主分支隨時穩定。

## 結論

本測試計畫全面涵蓋了「星際小可愛」專案從前端介面、後端核心到前後端協作的各個部分。透過**單元測試**鎖定細節函式錯誤、**元件與整合測試**保障模組對接正確、**E2E 測試**模擬真實用戶體驗，以及**CI/CD 自動執行**確保每次改動都經過驗證，我們可以大幅提升專案的可靠性與維護效率。在重構完成後立即補齊這些測試，能防止回歸問題，並為未來新增功能提供安全網。隨著開發進展，我們也會持續更新測試案例，做到**測試與程式碼同步演進**。最終，借助穩健的自動化測試與持續整合流程，開發者能更有信心地優化「星際小可愛」，將更佳的體驗帶給使用者。
