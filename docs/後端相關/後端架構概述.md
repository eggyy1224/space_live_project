# GitHub 專案 Space Live Project 後端架構分析 (基於最新 README.md 更新)

## 1. 架構設計

後端採用 **Python FastAPI** 框架開發，提供高效的同步與非同步 API 服務。整體程式碼採用模組化架構，將功能劃分為多個模組以實現關注點分離。主要的後端組成包括：

- **應用主程式**：`main.py` 負責啟動 FastAPI 應用並配置日誌記錄（透過 `utils/logger.py`）。在啟動時載入各項服務並將路由註冊到應用上。
- **API 路由模組**：位於 `api/endpoints/` 資料夾，使用 FastAPI 的 `APIRouter` 定義各個 HTTP 路由和 WebSocket 端點（例如 `health.py`, `speech.py`, `expressions.py`, `websocket.py` 等）。這些路由模組各司其職：健康檢查、語音處理、表情查詢，以及即時通訊等。
- **服務層（Services）**：位於 `services/` 資料夾，封裝核心業務邏輯功能。包含 **AI 核心**、**語音轉文字服務 (STT)**、**文字轉語音服務 (TTS)** 等模組。其中 **AI 核心** 是重點：
    - **AI 服務接口 (`AIService`)**: 作為一個向後兼容的適配器層，供 API 路由調用。
    - **健壯的 LangGraph 對話引擎 (`DialogueGraph`)**: 位於 `services/ai/dialogue_graph.py`，使用 `LangGraph` 的 `StateGraph` 管理對話流程與狀態 (`DialogueState`)。負責輸入預處理、動態提示選擇、調用 LLM、記憶整合、後處理和錯誤處理，並**生成情緒關鍵幀 (`emotional_keyframes`)**。
    - **增強版多層次記憶系統 (`MemorySystem`)**: 位於 `services/ai/memory_system.py`，管理 AI 的記憶。包括：
        - 短期記憶緩存。
        - 長期對話記憶 (使用 **ChromaDB** 持久化)。
        - 角色核心記憶 (使用 **ChromaDB**)。
        - 摘要記憶 (使用 **ChromaDB**，由 LLM 異步生成)。
        - 智能檢索 (向量搜索，如 MMR)。
        - 記憶過濾。
    - *SpeechToTextService*（語音轉文字服務）：調用 **Google Cloud Speech-to-Text** 將上傳的音訊內容轉換為文字。
    - *TextToSpeechService*（文字轉語音服務）：使用 **Google Cloud Text-to-Speech** 將 AI 回應文字合成語音並取得音檔及長度。
    - *EmotionAnalyzer*（情緒分析服務）：**（已移除）** ~~分析使用者文字中的情緒傾向，推斷表情與意圖。主要情緒表達由 `DialogueGraph` 決定。~~
    - ~~*AnimationService*（動畫服務）：~~ (已移除) **臉部表情和嘴型同步的計算邏輯已整合或轉移。後端主要負責生成 `emotionalTrajectory` 數據，前端負責渲染。**
- **核心設定模組**：`core/` 資料夾內含應用設定 (`core/config.py`) 與 Pydantic 資料模型 (`core/models.py`)。
- **工具模組**：`utils/` 資料夾提供共用的工具函式，例如 `constants.py` 定義預設表情對應的 Morph Target 常數（**注意：可能部分已過時**）、`logger.py` 封裝日誌設定等。

架構上，後端透過 **FastAPI** 提供 REST API 與 WebSocket 服務。各服務模組透過明確的介面互相調用。這種分層設計讓 **語音處理**、**AI 對話**、**動畫數據生成** 等邏輯彼此解耦，利於後續維護與擴充。**AI 核心** 是新的重點，透過 `LangGraph` 實現了更複雜和可擴展的對話邏輯（包含**情緒關鍵幀生成**），並利用 `ChromaDB` 實現了持久化和多層次的記憶管理。其他服務（STT, TTS）與 AI 核心協同工作，共同完成互動流程。

此外，系統在日誌、錯誤處理方面也有考量：例如在語音轉文字過程中發生錯誤時進行重試，並記錄警告日誌；若仍失敗則回傳帶有錯誤碼與友好訊息的 JSON 給前端，以提升可靠性。

## 2. 記憶系統設計與資料流動

**記憶系統**方面，專案已從先前不明確的記憶體存儲演進為**增強版多層次記憶系統 (`MemorySystem`)**，該系統利用 **ChromaDB** 作為向量數據庫來持久化儲存關鍵記憶：

- **長期對話記憶**: 儲存經過篩選的重要對話歷史。
- **角色核心記憶**: 儲存 AI 的身份、背景設定等。
- **摘要記憶**: 儲存由 LLM 定期異步生成的對話摘要，用於高效回憶長期關鍵信息。

此外，系統還包含**短期記憶緩存**以處理即時上下文。環境設定檔 `.env.example` 中應包含 `VECTOR_DB_PATH` 指向 ChromaDB 的儲存路徑。

**資料流動**方面，後端主要充當**中介整合**多種服務與前端之間的資料交換。後端處理流程，特別是透過 WebSocket 的文字對話，已由 **`DialogueGraph`** 主導：

- **文字對話流程**（透過 WebSocket）：
    1.  **輸入接收與預處理**: 後端 WebSocket 端點接收前端訊息，傳遞給 `DialogueGraph`。`DialogueGraph` 的初始節點可能進行輸入分析。
    2.  **記憶檢索**: `DialogueGraph` 調用 `MemorySystem`，結合當前輸入和對話狀態，從短期緩存、長期對話庫、角色核心庫和摘要庫中檢索相關記憶。
    3.  **動態提示構建**: 根據輸入分析、檢索到的記憶和當前角色狀態，選擇或生成適合的 LLM 提示模板。
    4.  **LLM 調用**: 將構建好的提示發送給大型語言模型，獲取回應。包含錯誤處理和重試機制。
    5.  **回應後處理與情緒關鍵幀生成**: `DialogueGraph` 的節點對 LLM 的原始輸出進行處理，並**分析生成包含情緒標籤和比例的關鍵幀 (`emotional_keyframes`)**。
    6.  **狀態更新**: 更新 `DialogueState`。
    7.  **（異步）記憶整合**: `MemorySystem` 可能觸發異步任務生成對話摘要。
    8.  **觸發下游服務**:\n        - *TextToSpeechService*: 將最終回應文本合成語音，獲取音頻和時長。
    9.  **資料回傳**: 透過 WebSocket 將整合結果傳回前端：
        -   推送 `type: "chat-message"` 包含回應文本和音頻 URL。
        -   推送 `type: "emotionalTrajectory"` 包含音頻時長和情緒關鍵幀數據。**（取代了舊的 `morph_update` 和 `lipsync_update`）**
    10. **記憶儲存**: `MemorySystem` 將相關的對話輪次存入長期對話記憶庫。

- **語音對話流程**（透過 REST API `/api/speech-to-text`）：
    1.  接收音訊 Base64。
    2.  調用 *SpeechToTextService* 轉錄文字。
    3.  將轉錄後的文字**輸入 `DialogueGraph`** 進行上述類似的處理流程。
    4.  獲取 `DialogueGraph` 處理後的最終回應。
    5.  調用 *TextToSpeechService* 合成語音。
    6.  將識別文字、AI 回應文本、語音 Base64、時長等以 JSON 回傳。**（注意：此流程目前不直接觸發 `emotionalTrajectory` 推送，前端可能需要根據收到的文本自行處理或等待後續交互觸發）**

資料傳遞仍以**結構化的 JSON** 進行。核心變化在於 AI 處理邏輯轉移到了可配置、有狀態的 `DialogueGraph` 中，**並負責生成 `emotionalTrajectory` 數據**，記憶管理也更持久化和智能化。**後端不再計算或發送舊的 `morph_update` 或 `lipsync_update` 消息。**

## 3. API 設計與串接方式

後端提供的 API 端點大致保持不變，但內部實現已更新以整合新的 AI 核心：

| API 路徑                                | 方法      | 功能說明                                                                                                                               |
| :-------------------------------------- | :-------- | :------------------------------------------------------------------------------------------------------------------------------------- |
| `/api/speech-to-text`                   | `POST`    | 接收音訊，進行 STT，然後將文字**交由 `DialogueGraph` 處理**以生成 AI 回應和 TTS。**不直接推送動畫數據。**                        |
| `/api/preset-expressions/{expression}` | `GET`     | 獲取預設表情配置 (用於前端直接應用)。                                                                       |
| `/api/health`                           | `GET`     | 健康檢查端點 (不變)。                                                                                             |
| `/ws` 或 `/ws/interactions`             | WebSocket | 即時雙向通訊端點。前端發送的訊息將**由 `DialogueGraph` 處理**，後端推送的回應 (`chat-message`) 和 **`emotionalTrajectory`** 源自 `DialogueGraph` 及 TTS 服務的輸出。 |

API 設計保持了對外的簡潔性，將複雜的內部處理流程（如 STT -> LangGraph -> TTS）封裝起來。核心的 AI 處理能力由 `LangGraph` 驅動，提供了更強大的流程控制、狀態管理和**情緒表達數據生成**能力。

## 4. 與前端的互動方式

後端與前端的互動方式（REST API + WebSocket）基本不變，但後端處理核心和推送的數據格式發生了變化：

- **HTTP API 互動**：主要用於一次性請求或獲取靜態數據。
    - **語音輸入**: 前端錄製完語音後，上傳至 `/api/speech-to-text`。後端完成 STT、`DialogueGraph` 處理、TTS 後，返回包含轉錄文字、AI 回應、合成語音等的 JSON 結果。**不包含動畫數據。**
    - **獲取預設配置**: 前端通過 `GET /api/preset-expressions/{expression}` 等取得默認表情參數，用於前端直接應用。
    - **健康檢查**: 前端可定期調用 `GET /api/health` 監控後端狀態。
- **WebSocket 即時互動**：用於持續對話和實時動畫同步。
    - **發送用戶輸入**: 前端將用戶的文字輸入通過 WebSocket 發送給後端。
    - **後端處理與推送**: 後端接收後，**送入 `DialogueGraph`**。`DialogueGraph` 執行流程生成最終回應和**情緒關鍵幀**。隨後，觸發 TTS。結果通過 WebSocket 推送給前端：
        -   推送 `type: "chat-message"` 包含回應文本和音頻 URL。
        -   推送 `type: "emotionalTrajectory"` 包含音頻時長和情緒關鍵幀數據。**（取代了舊的 `morph_update` 和 `lipsync_update`）**
    - **錯誤處理**: 若交互過程中發生錯誤，後端會通過 WebSocket 發送 `type: "error"` 的消息。

狀態協調依然重要。`DialogueGraph` 管理的 `DialogueState` 是後端狀態的核心，其變化（如 AI 的情緒）**通過生成的 `emotionalTrajectory` 間接傳遞給前端**，由前端負責解釋和渲染動畫，確保了前後端狀態同步和動畫表現。

總體而言，前端看到的接口變化不大，但後端實現了基於 `LangGraph` 的更智能 AI 核心，**並將動畫數據的重心從後端計算轉移到生成描述性的 `emotionalTrajectory`，由前端負責渲染。** 這種 REST 與 WebSocket 互補的設計，使得系統能同時滿足**一次性請求的可靠性**與**連續互動的即時性、沉浸感**。

**Sources:** 本分析內容是基於對該 GitHub 專案的 README、相關文檔及程式碼結構的理解進行整理。

## 系統架構圖 (Mermaid)

```mermaid
graph TD
    subgraph "外部交互 (External)"
        UI["前端UI (React/Three.js)"]
        STT_API["外部 STT API"]
        TTS_API["外部 TTS API"]
        LLM_API["外部 LLM API (e.g., Google GenAI)"]
    end

    subgraph "後端核心 (FastAPI)"
        Router["API 路由器 (endpoints)"]
        WebSocket["WebSocket 端點"]
        SpeechEndpoint["Speech 端點"]
        AIService["AI 核心服務"]
        TTS_Service["TTS 服務"]
        STT_Service["STT 服務"]
        DialogueGraph["對話圖 (LangGraph)"]
        MemorySystem["記憶系統"]
        CharacterState["角色狀態管理"]
        Logger["日誌記錄"]
    end

    UI -- WebSocket --> WebSocket
    UI -- HTTP (語音上傳) --> SpeechEndpoint
    
    WebSocket -- 接收消息 --> AIService
    SpeechEndpoint -- 請求處理 --> STT_Service
    SpeechEndpoint -- 請求處理 --> AIService
    SpeechEndpoint -- 請求處理 --> TTS_Service
    
    STT_Service -- 調用 --> STT_API
    STT_API -- 結果 --> STT_Service
    STT_Service -- 文本 --> SpeechEndpoint

    AIService -- 使用 --> DialogueGraph
    AIService -- 更新 --> CharacterState
    DialogueGraph -- 使用 --> MemorySystem
    DialogueGraph -- 調用 --> LLM_API
    DialogueGraph -- 更新 --> CharacterState
    LLM_API -- 結果 --> DialogueGraph
    MemorySystem -- 讀寫 --> CharacterState # 簡化表示，實際可能更複雜

    AIService -- 請求文本 --> TTS_Service
    TTS_Service -- 調用 --> TTS_API
    TTS_API -- 音頻數據 --> TTS_Service
    TTS_Service -- 結果 --> WebSocket
    TTS_Service -- 結果 --> SpeechEndpoint
    
    WebSocket -- 推送消息 (文本/軌跡/音頻URL) --> UI
    SpeechEndpoint -- 返回結果 (含音頻URL) --> UI

    AIService --> Logger
    DialogueGraph --> Logger
    TTS_Service --> Logger
    STT_Service --> Logger
    WebSocket --> Logger
    SpeechEndpoint --> Logger
    MemorySystem --> Logger
```

**圖表說明:**

- **前端 UI** 通過 WebSocket 與後端進行主要交互（發送聊天消息、接收回覆、情緒軌跡、音頻URL等），並通過 HTTP 上傳語音文件到 `Speech` 端點。
- **WebSocket 端點** 負責處理實時通信，將消息路由到 `AIService`。
- **Speech 端點** 處理語音上傳，調用 `STT_Service` 將語音轉為文本，然後調用 `AIService` 生成回應，再調用 `TTS_Service` 將回應轉為語音，最後返回結果給前端。
- **AIService** 是核心協調者，使用 `DialogueGraph` 來管理對話流程。
- **DialogueGraph** (基於 LangGraph) 內部包含多個節點，負責記憶檢索 (`MemorySystem`)、Prompt構建、調用 `LLM_API`、處理結果、生成情緒關鍵幀、更新 `CharacterState` 等。
- **TTS/STT 服務** 分別封裝了與外部 API 的交互。
- 所有主要組件都會將日誌信息發送到 **Logger**。
