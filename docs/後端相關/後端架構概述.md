本專案的後端採用 **FastAPI** 框架實作，結合語音辨識、文字生成與語音合成等服務，提供與虛擬太空人互動的 API。整體架構遵循**路由 (Routes)**、**控制器 (Controllers)**、**服務 (Services)**、**資料模型 (Models/DTO)** 等分層模組設計，方便維護與擴充功能。下圖說明後端主要組件與互動關係：

```mermaid
mermaid
CopyEdit
flowchart LR
    subgraph 使用者
        User[前端使用者<br/>（瀏覽器客戶端）]
    end
    subgraph 後端 FastAPI 應用
        direction TB
        RouteAPI[REST API 路由<br/>`/api/*`]
        RouteWS[WebSocket 路由<br/>`/ws`]
        ControllerAPI[API 控制器函式<br/>（在 endpoints 定義）]
        ControllerWS[WebSocket 處理函式<br/>（在 endpoints 定義）]
        RouteAPI --> ControllerAPI
        RouteWS --> ControllerWS
        subgraph Service 層
            STT[SpeechToTextService<br/>語音轉文字服務]
            TTS[TextToSpeechService<br/>文字轉語音服務]
            AI[AIService<br/>對話與記憶服務]
            Emotion[情緒分析元件]
        end
        ControllerAPI --> STT
        ControllerAPI --> AI
        ControllerAPI --> TTS
        ControllerWS --> Emotion
        ControllerWS --> AI
        ControllerWS --> TTS
        subgraph AI 子系統
            DG[DialogueGraph<br/>對話流程圖]
            Mem[MemorySystem<br/>記憶系統]
        end
        AI --> DG
        AI --> Mem
    end
    User -- 上傳語音/文字訊息 --> RouteAPI
    User -- 雙向通訊 --> RouteWS
    ControllerAPI -- 回傳文字與語音結果 --> User
    ControllerWS -- 推送對話回應與角色狀態 --> User

```

## 目錄結構與模組說明

後端程式碼位於 `prototype/backend/` 資料夾中，各子目錄與檔案功能說明如下：

```
plaintext
CopyEdit
backend/
├── api/                   # API 路由與控制器相關模組
│   ├── endpoints/         # 定義各個 API 路徑的控制器函式 (路由處理)
│   │   ├── health.py      # 健康檢查端點 (回應服務狀態)
│   │   ├── speech.py      # 語音上傳與文字轉語音的 REST API 端點
│   │   └── websocket.py   # WebSocket 即時通訊端點處理 (聊天與情緒更新)
│   ├── middleware/        # 中介軟體 (Middleware) 定義
│   │   └── cors.py        # CORS 設定模組（允許跨來源請求）
│   └── base.py            # FastAPI 應用建立函式 (create_app)，初始化應用與加入中介軟體
├── core/                  # 核心配置與資料模型
│   ├── config.py          # 統一的應用設定 (例如讀取環境變數，如 OpenAI API 金鑰等)
│   ├── exceptions.py      # 自訂例外與錯誤處理類別
│   └── models.py          # 基本資料模型定義 (使用 Pydantic，如請求與消息的結構)
├── dtos/                  # DTO (Data Transfer Objects) 模組，定義API請求/回應的資料結構
│   ├── requests.py        # 請求資料結構 (例如 SpeechToTextRequest、TextToSpeechRequest 等)
│   └── responses.py       # 回應資料結構 (尚未完整使用，預留擴充)
├── services/              # 核心服務層邏輯，處理主要功能與對外部服務的整合
│   ├── speech_to_text.py  # 語音轉文字服務 (封裝 OpenAI Whisper API 非同步呼叫)
│   ├── text_to_speech.py  # 文字轉語音服務 (封裝文字轉語音的處理，使用 OpenAI TTS 或其他引擎)
│   └── ai/                # AI 對話與記憶系統子模組
│       ├── dialogue_graph.py    # 定義對話流程圖與節點處理 (管理回應邏輯的流程)
│       ├── memory_system.py     # 角色記憶系統 (維護對話歷史、角色狀態等記憶資料)
│       ├── prompts.py           # 提示詞模版 (定義與生成 AI 模型使用的提示內容)
│       ├── graph_nodes/         # 對話圖各節點的實作 (輸入處理、LLM 互動、工具調用等節點)
│       ├── memory_components/   # 記憶相關組件 (如長短期記憶的資料結構與維護)
│       └── tools/               # AI 可用工具模組 (如外部資料查詢等，若有定義)
├── utils/                 # 工具與共用功能
│   ├── constants.py       # 常數定義 (如預設值等)
│   └── logger.py          # 日誌設定工具 (設定 logging 格式、等級等)
├── debug_audio/           # 上傳語音的除錯音檔儲存目錄 (執行時生成，用於存放收到的音訊)
├── .env.example           # 範例環境變數檔案 (包含需要設定的金鑰如 OPENAI_API_KEY 等)
├── requirements.txt       # Python 相依套件清單
└── main.py                # 後端服務啟動入口 (載入應用並運行 UVicorn 伺服器)

```

### 路由 (Routes) 與控制器 (Controllers)

- **API 路由 (`api/endpoints/`):** 使用 FastAPI 的 `APIRouter` 將不同功能的 API 端點分組定義。例如：
    - `health.py` 定義了健康檢查路由 `/api/health`，回傳簡單的狀態確認(`{"status": "ok"}`)。
    - `speech.py` 定義了 `/api/speech-to-text` 等 POST 路由，用於接受語音上傳請求，處理語音轉文字並產生回應。
    - `websocket.py` 定義了 WebSocket 端點的處理函式，用於雙向通訊（路徑通常為 `/ws`）。由於 WebSocket 非傳統 REST 路由，這裡透過 FastAPI 的 `app.add_websocket_route` 註冊。
- **控制器函式:** 路由對應的處理函式可視為控制器，負責協調請求資料的處理流程。它們會驗證輸入、呼叫服務層完成業務邏輯，並將結果封裝為 HTTP 回應或透過 WebSocket 發送給前端。例如：
    - `/api/speech-to-text` 的控制器接收到音訊檔後，會依序呼叫語音轉文字服務、AI 對話服務、文字轉語音服務，最終將辨識出的文字與 AI 回應（文字及語音）一起以 JSON 回傳。
    - WebSocket 控制器在偵測到來自前端的訊息時，會先解析消息類型，例如 `message`（使用者輸入的新文字訊息）或 `chat-message`（對話流程訊息），再根據內容呼叫情緒分析、AI 回應生成、語音合成等服務，通過 WebSocket 將結果逐步推送給前端。
- **中介軟體 (Middleware):** 在 `api/middleware/` 中定義了如 CORS 中介軟體的設定 (`cors.py`)。在應用啟動時會添加該中介軟體，使後端允許前端跨來源請求。CORS 設定允許所有網域（開發期間使用 `origins=["*"]`），並允許所有常用HTTP方法與標頭，確保前後端通訊不受同源政策限制。
- **應用啟動與路由註冊:** 在 `api/base.py` 中的 `create_app()` 函式建立 FastAPI 應用物件，設定標題等基本資訊，並載入預設的 CORS 中介軟體設定（透過 `CORSMiddleware` 允許特定來源，這部分在 `init_app` 時亦可調整）。主要的路由註冊發生於 `api/__init__.py` 的 `init_app()`：其中調用了 `create_app()` 建立應用後，使用 `app.include_router()` 將 `health` 與 `speech` 模組的路由註冊到主應用，並使用 `app.add_websocket_route("/ws", websocket.websocket_endpoint)` 註冊 WebSocket 處理函式。在 `main.py` 中匯入並呼叫 `init_app()` 取得設定完畢的 `app`，然後透過 Uvicorn 啟動服務。開發時 `reload=True` 熱重新載，可在程式變更時自動重啟伺服器。

### 資料模型 (Models) 與 DTO

- **核心資料模型 (`core/models.py`):** 使用 Pydantic 定義了後端處理所需的資料結構，例如:
    - `SpeechToTextRequest`：語音轉文字請求模型。如果以 JSON 形式提交 base64 音訊時會用到，包含 `audio_base64` 以及 `mime_type` 等欄位。
    - `WebSocketMessage`：WebSocket 通訊的消息模型，定義了消息的基本結構如類型 `type`、文字內容 `content` 或其他資料 `data`。
        
        核心模型側重在**後端內部**使用的資料結構以及部分對外API的請求定義。透過這些模型，FastAPI可自動驗證請求內容並解析為 Python 物件使用。
        
- **DTO (`dtos/requests.py` 與 `dtos/responses.py`):** 此模組提供**資料交換物件**定義，和 `core.models` 有些重疊。`requests.py` 中定義了如 `SpeechToTextRequest`、`TextToSpeechRequest`、`EmotionAnalysisRequest` 等請求格式，`responses.py` 預計放置回應格式的定義。在目前實作中，部分請求直接使用了 `core.models` 的定義（例如 `SpeechToTextRequest`），未來開發者可以視需要統一改用 `dtos` 中的模型以清晰區分「輸入輸出資料結構」與「內部資料模型」。DTO 的設計讓不同層之間傳遞的資料格式明確，如需修改 API 輸入輸出，只需調整相應 DTO 定義並同步修改控制器使用方式即可。
- **配置與例外 (`core/config.py` 與 `core/exceptions.py`):**
    
    `config.py` 負責讀取環境變數與配置應用參數。例如 `OPENAI_API_KEY`、CORS 允許的來源等。開發者可以在 `.env` 文件中設定這些值，`config.py` 讀取後提供給其他模組使用。也定義了一個 `Settings` 類別集中管理配置，未來可改用 Pydantic 的 `BaseSettings` 繼承以便自動解析環境變數。
    
    `exceptions.py` 定義了後端可能用到的自訂例外，如 `SpeechServiceException` 等，用於封裝第三方服務錯誤或業務邏輯錯誤，提供一致的錯誤訊息。在控制器中可捕捉這些例外並轉換為適當的 HTTP 錯誤回應碼與訊息。
    

### 服務層 (Services)

服務層模組封裝了主要的業務邏輯，讓控制器可以專注處理流程，而將具體功能委派給服務類別。這使程式更易讀，也方便日後替換底層實現（例如換用不同的語音識別服務）。本專案的服務主要包括：

- **SpeechToTextService (`services/speech_to_text.py`):**
    
    提供 **語音轉文字** 功能。利用 OpenAI Whisper API 實現語音識別：在初始化時讀取 `OPENAI_API_KEY` 建立 OpenAI 客戶端，並使用 Whisper 模型 (`whisper-1`) 來轉錄語音。
    
    `transcribe_audio(audio_data, mime_type)` 方法接受音訊位元資料與 MIME 類型，將音訊上傳到 OpenAI 的語音轉錄端點，取得轉錄結果文字。方法會回傳一個字典，包括`text`（轉出的文字）與`success`狀態等資訊。若 API 金鑰未提供或服務呼叫失敗，會適當記錄日誌並回傳錯誤訊息。開發者可以在此擴充對其他語音服務的支援，例如改用本地模型或不同雲服務，只需確保對外介面 (`transcribe_audio`) 不變。
    
- **TextToSpeechService (`services/text_to_speech.py`):**
    
    提供 **文字轉語音** 功能。當前實作使用了 OpenAI 提供的 TTS 功能（模型名稱如 `"gpt-4o-mini-tts"` 與特定語音 `"nova"`）。服務初始化時同樣使用 OpenAI 金鑰建立客戶端。
    
    `synthesize_speech(text)` 方法將輸入文字提交給 TTS 引擎產生語音，設定了一些參數如語速 (`speed=1.1`) 以及附加的 `instructions`（如角色設定或語調說明）。回傳結果包含音訊的 base64 字串以及估計的音訊長度（秒）。如果文字輸入為空或服務不可用，會回傳 None。未來可替換為其他 TTS 引擎（例如 Google TTS、Microsoft Azure TTS 等），只需修改這個服務的內部實作。預設的 `TTS_INSTRUCTIONS` 提供轉語音時的上下文說明，開發者可調整以改變語音表現風格。
    
- **AIService (`services/ai/__init__.py` 等):**
    
    提供 **AI 對話與記憶** 功能，是後端智慧的核心。它綜合運用大型語言模型（LLM）生成回應、維護對話狀態與角色記憶，並可整合情緒分析與工具使用。由於實作較複雜，組織在 `services/ai/` 子資料夾下：
    
    - `dialogue_graph.py` / `graph_nodes/`: 以**對話圖 (Dialogue Graph)** 的形式定義 AI 回應流程。可將對話流程拆解為節點，例如「處理使用者輸入」、「與 LLM 交互產生回覆」、「工具查詢節點」等，逐步處理使用者的請求。`graph_nodes` 中的各個檔案對應不同類型的節點實作，例如 `input_processing.py`（預處理輸入）、`llm_interaction.py`（與大語言模型交互）、`tool_processing.py`（調用外部工具如資料庫或API）、`analysis_nodes.py`（分析輸入內容）等。`tool_registry.py` 則可能維護可用工具清單。
    - `memory_system.py` / `memory_components/`: 實現**長短期記憶**機制，用於讓 AI 記住之前的對話內容或角色設定。`MemorySystem` 類別管理記憶庫，例如最近幾次對話、重要資訊。`memory_components` 內可能包含記憶項目的表示與操作，比如使用 Embeddings 紀錄事實、過往互動等，使 AI 回應時可以引用相關記憶。
    - `prompts.py`: 定義與組裝發給 AI 模型的提示詞 (prompt)。根據情境從記憶系統提取相關資訊，將使用者輸入、情緒狀態、角色人格等組合成完整的提示詞，傳給 LLM 以生成回答。開發者可以在此調整 Prompt 模版以優化 AI 的回應品質或添加約束。
    - **AIService 類別** 結合上述元件對外提供簡化的介面，例如 `generate_response(user_text, current_emotion=None)`：接收使用者輸入文字和目前情緒狀態，內部執行情境建構（參考 memory_system）、對話圖決策、調用 LLM 取得回應文字，同時更新角色的內部狀態(`character_state`)等。最後產生 AI 的文字回應結果（可能也更新記憶）。`AIService.character_state` 可用來追蹤 AI 人物的狀態或心情，在回應中提供更個性化的內容。由於 AIService 涉及非同步呼叫（例如等待 LLM 回應），其方法多為 `async`。未來開發者可以在此擴充更多能力（如更多樣的情緒狀態、工具使用等），或替換底層 LLM 模型。
- **情緒分析 (Emotion Analyzer):**
    
    雖未獨立成一個 services 檔案，但在 WebSocket 處理流程中，有一個 `emotion_analyzer` 物件用於對使用者文字進行情緒分析。推測其實作可能採用了簡單的方式（例如第三方套件或關鍵詞辨識）來推斷輸入文字的情緒傾向和置信度，用於調整虛擬角色的表情狀態。在 `websocket.py` 中可見：
    
    ```python
    python
    CopyEdit
    emotion, confidence = emotion_analyzer.analyze(user_text)
    
    ```
    
    並以置信度決定是否切換角色當前情緒狀態。情緒分析結果會影響 AIService 生成回應的語氣或內容，以及決定回傳給前端的角色表情更新。若未來需更精確的情緒分析，可將此部分獨立成服務模組並使用機器學習模型來實現。
    
- **日誌紀錄 (`utils/logger.py`):**
    
    後端啟動時會呼叫 `setup_logging()` 來配置日誌格式與等級。在關鍵操作（如呼叫外部 API、處理請求）處都有記錄 info 或 debug 級別的日誌，包含時間點與處理內容。這對開發者調試非常重要，例如在 `speech.py` 控制器中，上傳音訊後會記錄 "收到語音識別請求"、"成功讀取音頻數據(幾位元組)" 等訊息，以及在各服務開始/完成時記錄狀態。若發生錯誤，也有 logger.error 紀錄詳細原因。未來可以擴充 logger 使其將關鍵事件發送到監控系統或保存至檔案。
    

## 後端運作流程

下面分別說明 **REST API** 模式與 **WebSocket** 模式下，後端各模組協作處理請求的流程：

### 1. 語音辨識與回覆 (REST API 請求流程)

使用者透過前端（例如網頁）上傳語音檔給後端的 `/api/speech-to-text` 端點後，系統依序執行以下步驟：

1. **接收請求與內容檢查：** FastAPI 接收到 POST 請求後，`speech.py` 中對應的控制器函式 `process_speech_file` 會讀取請求的原始音訊內容。它首先檢查 Content-Type 是否為音訊格式，若缺失則假定為 WebM 格式並發出警告日誌。然後將請求體 (`request.body()`) 讀取為位元組串。如果音訊資料為空，立即回傳 400 錯誤給前端。
2. **除錯音訊保存：** 為了方便開發除錯，系統會將收到的音訊檔案存一份到伺服器本地 (`debug_audio/` 資料夾)，檔名帶有時間戳記與推斷的副檔名（根據 MIME 類型，例如 webm、ogg、wav、mp3）。這在開發或測試時可用來檢查上傳的內容是否正確無誤。若保存檔案失敗，僅記錄錯誤日誌但不影響後續流程。
3. **語音轉文字 (STT)：** 呼叫 `SpeechToTextService.transcribe_audio(audio_data, mime_type)` 將音訊發送給 OpenAI Whisper 模型進行轉錄。此為異步呼叫，控制器使用 `await` 等待結果。服務返回一個字典結果，例如 `{"text": "今天天氣很好", "success": True}`。控制器記錄轉錄結果文字內容以供除錯。
4. **AI 生成回應：** 如果語音識別成功且取得了非空文字，控制器接著使用 AIService 生成回應：
    - 取得使用者說的文字 `transcribed_text` 後，記錄該文字並傳入 `AIService.generate_response(user_text=transcribed_text)`。AIService 會基於目前的對話上下文與角色設定產生適當的回答文字。此外，AIService 內部也會更新角色的 `character_state` 或情緒，使回答連貫且具有人格特質。
    - `generate_response` 方法是異步的（因為可能涉及呼叫外部大語言模型 API），控制器透過 `await` 等待其完成並獲得 `response_dict`。該結果預期包含 AI 回覆的文字內容，以及可能的角色狀態資訊。
5. **文字轉語音 (TTS)：** 控制器取得 AI 回覆文字後，呼叫 `TextToSpeechService.synthesize_speech(ai_response)` 來將該回覆文字轉換為語音音訊。TTS 服務會返回一份結果，包括生成的語音資料（base64 編碼）以及預估的語音時長（秒）。如果 TTS 過程失敗或沒有產生語音（例如回答文字可能過空或服務不可用），則語音資料為 None，但流程不會中斷。
6. **回傳整合結果：** 最終，控制器將**使用者的語音識別結果**以及**AI的回應（文字與語音）**一起整合為 JSON 回應傳回前端。例如：
    
    ```json
    json
    CopyEdit
    {
      "userText": "今天天氣很好",
      "replyText": "是的，陽光明媚的日子讓人心情愉快！",
      "emotion": "happy",
      "audio": "<base64編碼的MP3音訊>",
      "hasSpeech": true,
      "speechDuration": 3.5,
      "characterState": {...角色內部狀態...}
    }
    
    ```
    
    - `userText` 可以是前端傳來音訊的轉譯文字，
    - `replyText` 是 AI 回答的文字，
    - `emotion` 代表 AI 虛擬角色此刻的情緒狀態，
    - `audio` 是回答語音的 base64 字串（前端可解碼播放），
    - `hasSpeech` 表示是否有語音回覆（若 TTS 失敗則為 false），
    - `speechDuration` 是語音長度，用於前端同步嘴型或其他時間控制，
    - `characterState` 則是AI角色目前的狀態資訊（例如心情指數或記憶摘要等，取決於 AIService 的設計）。
    
    FastAPI 會將此 Python 字典自動轉為 JSON 格式，回傳 HTTP 200 回應。前端接收到後即可將文字顯示並播放語音，實現與虛擬人聲的互動。
    

### 2. 即時對話 (WebSocket 雙向通訊流程)

除了傳統的 HTTP 請求外，後端也提供 WebSocket 接口讓前端與 AI 進行更即時、連續的交流。當使用者與虛擬太空人進行聊天時，通常會建立一條 WebSocket 連線 (`/ws`) 來持續傳遞訊息和狀態。WebSocket 流程如下：

1. **建立連線：** 前端連線到 `ws://<server>/ws`，後端 `websocket.py` 中的 `websocket_endpoint` 函式被觸發執行。首先，後端透過 `ConnectionManager` 對象的 `connect()` 方法接收該 WebSocket 連線（接受連線並加入活動連線列表）。伺服器端會記錄「connection open」以確認新的 WebSocket 已連接。
2. **初始化狀態：** 在進入消息處理循環前，後端為該連線初始化一些狀態變數，例如：
    - `current_emotion = "neutral"`：設定角色當前情緒為中立，
    - `emotion_confidence = 0.0`：當前情緒的置信度。
    這些狀態將用來在多輪對話中平滑地調整角色表情，不會每句話都劇烈改變情緒。
3. **等待並處理客戶端消息：** 後端進入一個 `while True` 迴圈，不斷等待 WebSocket 傳來的新訊息：
    - 使用 `data = await websocket.receive_text()` 獲取使用者透過 WebSocket 發來的文字訊息（這通常是前端封裝為 JSON 的字串）。
    - 透過 `json.loads(data)` 將文字解析為 Python 字典 `message`，根據其中的 `type` 欄位判斷訊息類型。系統定義了多種消息類型以處理不同情況：
        - **`type`: `"message"`** – 一般的使用者訊息，包含使用者輸入的內容，在普通聊天場景使用。
        - **`type`: `"chat-message"`** – 來自前端的對話訊息（可能用於測試性能，或特定格式的對話事件）。
        - 其他類型例如 `"morph_update"`（可能是臉部表情數值更新）、`"emotionalTrajectory"`（情緒變化軌跡）等，在程式中有提及但註解掉的部分，可在未來拓展。
4. **情緒分析與狀態更新：** 如果消息類型是一般使用者`message`，後端會：
    - 提取使用者文字內容 `user_text = message["content"]`。
    - 調用情緒分析元件，如 `emotion, confidence = emotion_analyzer.analyze(user_text)`，得到此文字所代表的情緒類型以及信心水準。
    - 根據得到的 `confidence` 與當前的 `emotion_confidence` 比較：只有當新分析的情緒置信度更高時，才認定角色情緒需要切換為新的類型，否則維持原先的 `current_emotion`。這避免了情緒在不確定的狀況下頻繁跳動。隨後更新 `current_emotion` 和 `emotion_confidence` 為新的值或維持舊值。
    - （以上邏輯確保角色情緒呈現具有慣性與穩定性，提升互動體驗。）
5. **AI 回應生成：** 在決定了當前角色情緒後，使用者的文字會交由 AIService 生成回應：
    - `ai_response = await ai_service.generate_response(user_text, current_emotion)`：將使用者輸入和目前情緒狀態傳入 AIService，以獲得 AI 的文字回覆。AIService 能夠考慮情緒來調整回答內容（例如帶入對應語氣）。同時 AIService 內部也會更新對話記憶與角色狀態。
    - 一旦拿到 AI 回覆文字 `ai_response`，後端緊接著調用語音合成服務：
    - `tts_result = await tts_service.synthesize_speech(ai_response)`：將 AI 回覆文字轉換為語音。得到的結果包含 `audio`（base64音訊）及 `duration`（音訊時長）。若合成失敗，`tts_result` 可能為 None，後續會根據這種情況調整回傳內容。
6. **通過 WebSocket 回傳回應：** 後端使用 `websocket.send_json(...)` 將回覆發送給前端。針對一般 `message` 類型的請求，回傳的 JSON 包含：
    - `"type": "response"`：標識這是一則回應訊息，
    - `"content": ai_response`：AI回覆的文字內容，
    - `"emotion": current_emotion`：此刻角色的情緒，
    - `"confidence": emotion_confidence`：該情緒的置信度，
    - `"audio": audio_base64`：AI回覆語音的base64字串（如果有生成），
    - `"hasSpeech": bool(audio_base64)`：是否有語音內容，
    - `"speechDuration": audio_duration`：語音長度（如果有語音，否則可為預估值），
    - `"characterState": ai_service.character_state`：附帶角色當前狀態（例如情緒指數、記憶摘要等，由AIService維護）。
        
        這封裝了AI對使用者此輪發話的完整回應，包括文字和聲音，前端拿到後可同步顯示文字、播放語音並更新角色的表情狀態。
        
7. **其他消息類型的處理與回應：** 若接收到不同 `type` 的消息（例如 `"chat-message"`），`websocket_endpoint` 內有相應處理邏輯：
    - `"chat-message"`：表示前端傳來了一段使用者對話，鍵名可能不同（程式中使用 `message["message"]` 取得內容）。後端流程與 `"message"` 類似，也是生成AI回答並透過WebSocket回傳，不過在程式中可能增加了效能計時（例如記錄收到請求到回應送出的各時間點，以分析延遲）。
    - 針對前端不同需求，後端可以回傳額外的訊息。例如在程式中有提到：
        - `"type": "emotionalTrajectory"`：也許用來傳送一段時間內情緒變化數據給前端，讓前端可以平滑呈現表情過渡。
        - `"type": "character_state_update"`：用於在不發送對話的情況下，單獨更新角色當前狀態給前端，比如每隔一段時間同步角色心情或健康狀況。
        - `"type": "error"`：如果處理過程中拋出異常（例如 AIService 或 TTS 發生錯誤），後端會捕捉 `WebSocketDisconnect` 或其他 Exception，並透過類型為 error 的消息通知前端發生錯誤，內容包含錯誤原因，前端可據此提示使用者或嘗試重連。
        這些額外類型提高了通訊的靈活性。開發者在擴充功能時，可以自訂新的消息類型，在前後端協商好格式後實現更多即時互動的功能。
8. **連線關閉處理：** 若使用者關閉前端或網路中斷，`websocket.receive_text()` 會引發 `WebSocketDisconnect` 異常。後端在 `except WebSocketDisconnect` 中使用 `manager.disconnect(websocket)` 將該 WebSocket 從活動連線列表移除，並結束該連線的處理循環。伺服器端日誌會記錄連線斷開的事件。對於中斷的連線，程式中會跳出循環結束該 `websocket_endpoint` 協程。如此可釋放資源且不影響其他連線。前端若需要，可選擇重新連線並從最近的對話狀態繼續（具體續連邏輯需自行實現，例如每次建立新連線時讓AIService載入先前記憶）。

透過 WebSocket，前端與後端可以進行更順暢的雙向交流：使用者發一句話，AI 立即透過同一連線回傳回應，不需每次 HTTP 請求開新連線，同時可以持續推送如情緒變化、動作指令等資訊，達到即時互動效果。

## 總結

後端架構將請求處理的各個階段模組化：路由與控制器負責接收請求和回傳結果，服務層執行具體的 AI 與多媒體處理，資料模型統一請求與回應的結構，核心配置集中管理設定。一個完整請求從進入後端到回傳，會經過**路由 → 控制器 → 服務 (可能串連多個服務) → 控制器組裝回應 → 回傳**的流程；而 WebSocket 連線則在建立後持續觸發**接收消息 → 處理 → 回傳**循環。

這樣的分層設計使後續開發者能夠清晰地定位功能位置並進行維護：例如，若要新增一個分析使用者上傳圖片的功能，可以新增一個 `api/endpoints/image.py` 控制器與對應的 `services/image_analysis.py` 服務，不會影響現有語音功能；修改 AI 回應風格時，可調整 `services/ai/prompts.py` 或 `AIService` 的邏輯，而無需改變 API 路由。

**維護性提示：** 開發者應保持各模組單一職責、低耦合。例如控制器中避免寫過多處理細節，將複雜邏輯下沉到服務；服務實現中如需使用外部金鑰或設定，從 `core.config` 取得，不要硬編碼；新增文件或資源時，遵循目錄結構放置（如音訊檔放入 `backend/audio/`，並利用已有的靜態路由 `/audio` 提供存取）。同時注意更新相關文件（如 API 文件或 README）。

藉由上述架構與說明，未來開發者可以快速理解專案後端的運作方式，並在此基礎上安全地擴充功能或進行優化，確保專案的可維護性和持續演進。