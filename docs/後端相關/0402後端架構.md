# 後端架構設計文檔

## 1. 架構設計

後端採用 Python FastAPI 框架開發，提供高效的同步與非同步 API 服務。整體程式碼採用模組化架構，將功能劃分為多個模組以實現關注點分離。

主要的後端組成包括：

### 應用主程式
- `main.py` 負責啟動 FastAPI 應用並配置日誌記錄（透過 `utils/logger.py`）
- 在啟動時載入各項服務並將路由註冊到應用上

### API 路由模組
- 位於 `api/endpoints/` 資料夾
- 使用 FastAPI 的 APIRouter 定義各個 HTTP 路由和 WebSocket 端點
- 主要路由模組：health.py, speech.py, expressions.py, websocket.py 等
- 各模組各司其職：健康檢查、語音處理、表情查詢，以及即時通訊等

### 服務層（Services）
位於 `services/` 資料夾，封裝核心業務邏輯功能。每個服務通常在應用啟動時以單例模式初始化，可在各路由中共用。主要服務包括：

- **AIService**（AI 對話服務）：與雲端 AI 平台（如 Google Gemini AI 或 OpenAI API）整合，根據使用者輸入產生對話文字回應
- **SpeechToTextService**（語音轉文字服務）：調用 Google Cloud Speech-to-Text 將上傳的音訊內容轉換為文字
- **TextToSpeechService**（文字轉語音服務）：使用 Google Cloud Text-to-Speech 將 AI 回應文字合成語音並取得音檔及長度
- **EmotionAnalyzer**（情緒分析服務）：分析使用者文字中的情緒傾向，推斷表情與意圖
- **AnimationService**（動畫服務）：根據情緒產生對應的臉部表情 Morph Target 數值，並提供表情過渡和平滑插值計算，用於角色嘴型與表情動畫同步

### 核心設定模組
- 位於 `core/` 資料夾，包含應用設定與資料模型
- `core/config.py` 定義應用參數（如表情過渡速度、步驟等）
- `core/models.py` 定義 Pydantic 資料模型（如語音請求模型、WebSocket 訊息模型等）供請求與回應時使用

### 工具模組
- 位於 `utils/` 資料夾，提供共用的工具函式
- `constants.py` 定義預設表情對應的 Morph Target 常數
- `logger.py` 封裝日誌設定等

架構上，後端以 FastAPI 提供傳統 REST API 與 WebSocket 即時服務。各服務模組透過明確的介面互相調用，例如 WebSocket 端點會調用 AIService、EmotionAnalyzer 等以完成一個完整互動流程。這種分層設計讓語音處理、AI 對話、表情動畫等邏輯彼此解耦，利於後續維護與擴充。

此外，系統在日誌、錯誤處理方面也有考量：如在語音轉文字過程中發生錯誤時重試最多3次，並記錄警告日誌；若仍失敗則回傳帶有錯誤碼與友好訊息的 JSON 給前端，以提升可靠性。

## 2. 資料庫設計與資料流動

### 資料庫設計
專案目前並未使用傳統關聯式或NoSQL資料庫來持久儲存大量資料。後端的互動紀錄主要在記憶體中處理，暫無明確的資料庫 Schema 定義。專案的環境設定檔 `.env.example` 僅提供了 DATABASE_URL 佔位符，而未指定具體資料庫類型。這表示團隊預留了資料庫配置的可能性，但當前實作仍以即時處理為主，尚無複雜的資料庫結構。

### 資料流動
後端主要充當中介整合多種雲端服務與前端之間的資料交換，資料流大致如下：

#### 文字對話流程（透過 WebSocket）
使用者在前端輸入文字（或經語音識別得到文字），前端經由 WebSocket 將此文字發送給後端。後端接收到訊息後，會依序執行：

1. **情緒分析**：利用 EmotionAnalyzer 模組分析文字內容，判斷文字所表達的情緒及其置信度。例如可能得到情緒 "happy" 以及一定的信心值。

2. **表情決策**：比較新情緒的置信度與當前表情狀態，決定是否切換表情。如新情緒更強烈，則選擇新情緒，否則維持原表情。接著使用 AnimationService 計算目標表情對應的 Morph Target 值，並生成當前表情過渡到目標表情的中間值（Transition morph）以實現平滑過渡。

3. **AI 回應生成**：將使用者文字和當前情緒傳入 AIService，調用雲端大型語言模型（如 Google Gemini 或 OpenAI ChatGPT）生成文字回應。

4. **語音合成**：將 AI 回應文字交由 TextToSpeechService，透過 Google Cloud TTS 生成對應的語音音頻。服務返回音頻的 Base64 編碼串以及預估的語音時長（秒數）。若 TTS 失敗，後端會記錄錯誤並僅返回文字結果。

5. **嘴型同步計算**：動畫服務根據回應文字和語音時長，計算出一系列嘴部動作的關鍵帧（lipsync frames），以驅動3D角色嘴型隨語音開合的動畫。

6. **資料回傳**：後端通過 WebSocket 將整合的結果資料以 JSON 格式傳回前端：
   - 首次回傳的訊息包含：AI文字回應、當前情緒、表情Morph數值、語音音頻（Base64）、語音時長等
   - 後續持續推送表情過渡更新和嘴型同步的資料點：
     - **表情過渡更新（morph_update）**：將從初始表情逐步過渡到目標表情的中間Morph數值按設定的步驟和延遲連續傳送
     - **嘴型同步更新（lipsync_update）**：按照語音時長，分多幀傳送嘴型動畫的Morph數據

#### 語音對話流程（透過 REST API）
後端也提供傳統 POST 端點來處理一次性的語音輸入請求。前端將錄製的音訊（如 WebM/Opus 格式）以 Base64 字串和 MIME 類型封裝成請求 JSON 發送到 `/api/speech-to-text`。後端接收後的流程是：

1. 解碼音訊 Base64 並暫存檔案以供除錯，確認音訊格式
2. 調用 SpeechToTextService 的 transcribe_audio 方法，將音檔送至 Google STT 服務獲得轉錄文字及信心指數
3. 若轉錄成功，使用 AIService 產生對應的文字回應；然後用 TTS 服務合成語音
4. 最後將結果以 JSON 回傳：包含識別出的文字、AI文字回應、語音音檔（Base64）、音檔長度，以及 STT 信心指數等

在這些資料流過程中，所有的資料傳遞都是結構化的 JSON：無論是 REST 回應或 WebSocket 訊息，都以明確的欄位傳遞內容、狀態和錯誤資訊。例如，錯誤情況下會有 `{"error": {"code": "...", "message": "友善提示"}}` 的結構。

由於目前沒有複雜的資料庫操作，資料主要在記憶體中短暫存在，用完即釋放或由週期性程序清理（如計畫中所述對語音檔案建立自動清理機制，每週清理過期檔案）。這樣的無狀態設計也方便水平擴展和維護系統狀態的一致性。

## 3. API 設計與串接方式

後端提供的 API 端點遵循 RESTful 風格進行設計，路由命名語意清晰並使用適當的 HTTP 方法來區分動作。根據實際實作，主要的 API 如下：

| API 路徑 | 方法 | 功能說明 |
|---------|------|----------|
| `/api/speech-to-text` | POST | 處理語音轉文字請求，並生成AI回應。接收包含 audio_base64 和 mime_type (audio/webm;codecs=opus) 的JSON請求體，將用戶語音轉為文字並產生AI回應。 |
| `/api/preset-expressions/{expression}` | GET | 獲取預設表情配置。根據路徑參數expression返回對應的表情參數設置，用於前端設定模型表情。 |
| `/api/health` | GET | 健康檢查端點。用於確認後端服務狀態是否正常運作，前端可透過此端點監控系統可用性。 |
| `/ws/interactions` | WebSocket | 即時雙向通訊端點，前端透過 Socket 連至此路徑進行互動，發送用戶輸入並接收即時回應與動畫更新。 |

以上路由中，`/api/*` 為傳統 RESTful API，`/ws/*` 為 WebSocket 即時服務。命名風格採用了名詞或動賓短語來表達資源或動作，例如「speech-to-text」直觀表示語音轉文字操作。每個端點嚴格區分 GET/POST 方法：查詢操作使用 GET，提交處理請求使用 POST，符合 RESTful 慣例。回應內容皆為 JSON 結構，且在正常與錯誤情況下結構固定，便於前端解析。

在設計上，API 考慮了簡潔與易用性。例如 `/api/speech-to-text` 端點將語音處理、AI對話與回應生成封裝在一個請求中，前端只需提供音訊資料即可獲得完整的處理結果。錯誤處理方面，系統採用統一的錯誤回應格式，特別是驗證錯誤時會返回結構化的 detail 數組，包含具體的錯誤位置（loc）、錯誤訊息（msg）和錯誤類型（type），便於前端精確識別問題所在。

專案未使用 GraphQL 等查詢語言。所有資料提供經由 REST API 或 WebSocket 傳遞，屬於典型的 RESTful 後端架構。這使系統結構清晰、調用簡單，同時透過 WebSocket 滿足了即時性需求。

## 4. 與前端的互動方式

後端與前端透過 REST API 及 WebSocket 兩種方式互動，各自承擔不同任務，以實現既即時又可靠的用戶體驗。

### HTTP API 互動
對於非即時性或啟動階段的請求，前端會呼叫後端的 REST API：

- 應用載入時，前端可能透過 GET `/api/preset-expressions/neutral` 等取得默認表情參數，用於初始化3D模型表情
- 使用者選擇語音輸入模式時，前端錄製完語音後會以 POST 請求將音訊上傳至 `/api/speech-to-text`，等待後端返回完整的文字與語音結果
- 前端也可定期呼叫 `/api/health` 監控後端狀態，在偵測到服務異常時提示用戶

REST API 通常在一次請求中返回完整資料，前端拿到回應後即可更新UI，適合一次性請求/回應的場景。

### WebSocket 即時互動
在持續對話與動畫同步方面，前端使用 WebSocket 與後端保持長連線（連線路徑為 `/ws/interactions`）。一旦連線建立，雙方即可進行全雙工通信：

1. 前端每當用戶提交新問題（文字輸入或經語音識別轉成的文字）時，會透過 WebSocket 發送一則 JSON，例如：`{ "type": "message", "content": "你好嗎？" }`

2. 後端處理後的資料流向：
   - **首次回應**：發送 `type: "response"` 的消息，包含 AI 回覆的文字、當前情緒狀態、表情Morph數據以及語音的Base64和時長等
   - **表情過渡**：異步任務每隔幾十毫秒推送 `morph_update` 消息，內含漸變中的 morphTargets 值和對應情緒
   - **嘴型同步**：按照音節節奏推送 `lipsync_update` 消息，每條消息帶有當前口型的 morphTargets 值

3. 錯誤處理：若在交互過程中發生錯誤，後端會通過 WebSocket 發送 `type: "error"` 的消息以及錯誤內容。前端監聽這類錯誤訊息，收到後會在介面上提示使用者，同時可以決定是否自動重試或斷線重連。

4. 狀態協調：前後端在即時通信中採取狀態協調措施。後端透過每次回應通知前端當前的情緒狀態和表情，使前端模型狀態與後端邏輯同步一致。

總體而言，前端透過 REST API 獲取靜態資料或進行單次請求，透過 WebSocket 實現連續對話與即時更新。兩者互補：REST確保了交互的可靠性和簡單性，即時通道則帶來了互動的高及時性和沉浸感。這種設計使得虛擬宇航員系統能同時滿足實時反應與狀態可控的需求。

在未來擴充時，計畫增強前後端互動的韌性，例如實現自動重連機制、服務降級備援等，以進一步提升系統在不穩定網路環境下的表現。