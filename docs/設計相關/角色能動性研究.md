# 具有主動性與能動性的虛擬太空主播技術方案

## 1. 角色能動性設計

- **技術實作方式與推薦框架:** 利用 **行為驅動的對話管理** 賦予 AI 角色內在驅動力。可採用 LangChain 的 **LangGraph** 架構具狀態的對話流程圖，引入狀態節點與決策節點來控制角色行為。後端搭配大型語言模型作為角色的大腦（如 OpenAI GPT-4 或 Google **Gemini 2.0**），模擬角色的思考與情感表達。Gemini 2.0 等次世代多模態模型具備原生處理文字、語音和圖像的能力，能讓角色同時展現語言對答和情緒變化。在邏輯控制上，可結合**行為樹 (Behavior Tree)** 或 **有限狀態機 (FSM)** 管理角色情緒及動作邏輯，確保角色行為合理且一致。
    
    [github.com](https://github.com/eggyy1224/space_live_project/blob/main/README.md#:~:text=,%E5%8B%95%E6%85%8B%E6%8F%90%E7%A4%BA%E6%A8%A1%E6%9D%BF%3A%20%E6%A0%B9%E6%93%9A%E5%B0%8D%E8%A9%B1%E6%83%85%E5%A2%83%EF%BC%88%E6%AD%A3%E5%B8%B8%E3%80%81%E6%BE%84%E6%B8%85%E3%80%81%E9%8C%AF%E8%AA%A4%EF%BC%89%E9%81%B8%E6%93%87%E4%B8%8D%E5%90%8C%E7%9A%84%E6%8F%90%E7%A4%BA%E6%A8%A1%E6%9D%BF%E3%80%82)
    
    [blog.google](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#:~:text=multimodal%20inputs%20like%20images%2C%20video,defined%20functions)
    
- **架構設計建議:** 後端維護一個**角色狀態對象**，包含角色的情緒（如快樂/沮喪指數）、能量值、目標傾向等內在參數。每次對話或事件後根據內容更新該狀態（例如根據用戶讚美提升愉悅值）。對話引擎（例如由 LangGraph 實現的 `DialogueGraph`）在生成回應時將此狀態作為額外上下文，影響回應的語氣與內容，使 AI 能自主表達內心情緒與觀點。前端則根據後端傳來的狀態信息，驅動 3D 模型表情與動作同步表現情緒（如愉悅時微笑、驚訝時瞪大眼）。前後端透過 WebSocket 實現**狀態-表情的即時聯動**，確保角色語音內容與表情動作協調一致。
    
    [github.com](https://github.com/eggyy1224/space_live_project/blob/main/README.md#:~:text=,%E9%8C%AF%E8%AA%A4%E8%99%95%E7%90%86%E8%88%87%E9%87%8D%E8%A9%A6%3A%20%E5%8C%85%E5%90%AB%20LLM%20%E8%AA%BF%E7%94%A8%E9%87%8D%E8%A9%A6%E5%92%8C%E6%A2%9D%E4%BB%B6%E8%B7%AF%E7%94%B1%E6%A9%9F%E5%88%B6%E3%80%82)
    
    [github.com](https://github.com/eggyy1224/space_live_project/blob/main/README.md#:~:text=%25%25%20%E5%89%8D%E7%AB%AF%E6%9C%8D%E5%8B%99%E8%88%87%E5%BE%8C%E7%AB%AF%E4%BA%A4%E4%BA%92%20CS%20,WS_Server)
    
- **功能模組清單:**
    - **PersonaProfile 模組：** 保存角色背景故事、個性設定與專業知識，用於生成對話時提供角色自我的語氣和立場依據。
    - **EmotionEngine 模組：** 實時追蹤與更新角色內在情緒狀態。根據對話內容或記憶變化調整角色心情，例如遭到反駁時降低愉悅度、長時間無互動時增加寂寞感。
    - **BehaviorScheduler 模組：** 行為排程器，定期檢查角色狀態和環境事件，決定是否觸發自主行為（如嘆氣、伸懶腰等閒置動作或主動發話）。確保角色能動性行為以合理頻率發生。
    - **DialogueManager 模組：** 對話管理器，負責與 LLM 交互生成文字回應，並結合 EmotionEngine 的狀態調整回應風格。同時管理對話流程狀態，處理正常回應或進入特定情境（如情緒爆發）的特殊流程。
        
        [github.com](https://github.com/eggyy1224/space_live_project/blob/main/README.md#:~:text=,%E5%8B%95%E6%85%8B%E8%A7%92%E8%89%B2%E7%8B%80%E6%85%8B%3A%20%E5%BD%B1%E9%9F%BF%20AI%20%E7%9A%84%E5%9B%9E%E6%87%89%E9%A2%A8%E6%A0%BC%E3%80%81%E8%A8%98%E6%86%B6%E6%AA%A2%E7%B4%A2%E7%AD%96%E7%95%A5%E7%AD%89%E3%80%82)
        
    - **AnimationController 模組：** 動作控制器，根據 DialogueManager 輸出的情緒和行為指令，在前端觸發虛擬主播模型對應的表情變化與肢體動作，體現角色自主的行為表現。
- **可整合的外部 API:**
    - **知識庫/資訊 API：** 結合維基百科或 NASA 開放資料 API，讓角色能查詢太空知識或最新新聞作為自身觀點的素材，強化角色在討論專業話題時的主動性與可信度。
    - **情感分析 API：** 利用 IBM Watson Tone Analyzer 等服務分析文字語調，協助 EmotionEngine 更精準地更新角色情緒（例如辨識用戶語氣是否生氣或開心）。
    - **LLM 雲服務：** 可串接 OpenAI 或 Google 的雲端 LLM（如 GPT-4 或 Gemini），利用其強大生成能力為角色產生豐富的內心獨白和情感化對話內容。

## 2. 主動互動能力

- **技術實作方式與推薦框架:** 採用**事件驅動**與**計時任務**相結合的方式，使角色不依賴用戶輸入也能啟動互動。後端建立一套 **Idle Trigger 機制**，當檢測到用戶長時間未輸入時，觸發角色主動對話流程。此流程可在 LangGraph 的對話圖中實作特殊節點：當進入「無用戶輸入」狀態時，由節點調用 TopicGenerator 模組（見下）生成新話題。配合 Python 任務調度（如 `APScheduler`）定時檢查條件，或在 WebSocket 連線空閒一定時間後由後端直接推送一條主動對話訊息。LLM 在這裡用於生成主動對話的內容，例如主動提出問題、分享趣聞等，讓互動自然展開。
    
    [github.com](https://github.com/eggyy1224/space_live_project#:~:text=,%E8%AE%93%20AI%20%E8%83%BD%E5%9F%BA%E6%96%BC%E8%A8%98%E6%86%B6%E5%92%8C%E7%8B%80%E6%85%8B%EF%BC%8C%E4%B8%BB%E5%8B%95%E7%99%BC%E8%B5%B7%E8%A9%B1%E9%A1%8C%E6%88%96%E6%8F%90%E5%95%8F%E3%80%82)
    
- **架構設計建議:** 引入**主動對話觸發服務**，持續監控多種觸發條件： (a) 時間閾值（如幾分鐘無對話）, (b) 角色內部狀態（如角色的“聊癢”值達高點）, (c) 環境線索（如觀眾出現新表情）。一旦條件滿足，觸發服務通知 DialogueManager 進入主動互動模式，可能使用不同的提示詞模板引導 LLM 產生開場白或提問。後端透過 WebSocket 將此主動對話的文本與表情指令推送給前端，前端像處理一般對話一樣顯示文字並播放語音/動畫。為避免打斷用戶，系統可在前端 ChatService 設計**上下文感知**：只有當目前沒有進行中的用戶語音輸入且先前回應已結束時，才允許插入主動對話。此外，可在控制面板提供參數讓用戶調整主動對話頻率或關閉該功能，以提升體驗友好度。
- **功能模組清單:**
    - **IdleMonitor 模組：** 閒置監控器，追蹤最後一次用戶輸入的時間戳並定期更新閒置時長。當閒置超過預設閾值時，向系統發出「空閒」事件。
    - **TopicGenerator 模組：** 主動話題生成器，基於角色記憶和當前上下文，自主產生下一個對話話題或問題。它會調用 LLM，傳入角色的興趣領域或近期對話摘要，生成適合當前情境的新話題（例如：「對了，我今天在太空站看到一件有趣的事…」）。
    - **ProactiveDialogue Orchestrator 模組：** 主動對話編排器，接收 IdleMonitor 的空閒事件後協調啟動 TopicGenerator，並將生成的主動話題傳給 DialogueManager 執行。負責保證主動對話與現有對話流銜接順暢，例如避免剛好在用戶開始說話時打斷。
    - **ScheduleManager 模組：** 排程管理器，使用定時任務框架維持定期任務（如整點問候、週期性播報）。可設定特定時間點觸發特殊互動（例如每日首次啟動時，主播主動問好並簡介今天的太空新聞）。
- **可整合的外部 API:**
    - **NASA 資料 API：** 利用 NASA 的開放API（如每日天文圖 Astronomic Picture of the Day、火星探測器圖片）作為主動內容來源。主播閒聊時可主動提及並展示NASA 最新發現或美圖，增添話題趣味。
        
        [github.com](https://github.com/eggyy1224/space_live_project#:~:text=,%E8%AE%93%20AI%20%E8%83%BD%E5%9F%BA%E6%96%BC%E8%A8%98%E6%86%B6%E5%92%8C%E7%8B%80%E6%85%8B%EF%BC%8C%E4%B8%BB%E5%8B%95%E7%99%BC%E8%B5%B7%E8%A9%B1%E9%A1%8C%E6%88%96%E6%8F%90%E5%95%8F%E3%80%82)
        
    - **新聞資訊 API：** 接入太空領域新聞源或 RSS，定期抓取簡要新聞。在長時間無人互動時，角色可主動播報一則當天的太空新聞或分享一條科學小知識。
    - **問答知識庫 API：** 結合維基百科或 Wolfram Alpha 等，在觀眾反應冷淡時由角色主動提出問題並立即給出答案（扮演小測驗遊戲），以此喚起觀眾參與。例如提問「你知道國際太空站現在有幾名宇航員嗎？」然後稍後揭曉答案。

## 3. 多模態整合策略

- **技術實作方式與推薦框架:** 打造**多模態交互管線**，融合語音、視覺和文本資訊。前端使用 **React + Three.js** 搭配 **React Three Fiber** 進行 3D 角色渲染，並透過瀏覽器錄音接口捕捉用戶語音。語音輸入經由 WebSocket 或 WebRTC 實時傳送至後端的 **語音識別模組**，採用如 **OpenAI Whisper** 或 Google Speech-to-Text 等引擎將語音轉為文字。視覺輸入方面，利用攝影機捕獲畫面影像，可每隔幾幀提取一張圖並通過 REST API 上傳後端進行 **計算機視覺** 分析（如人臉識別、物件識別）。語音輸出則由後端 **語音合成模組** 調用 TTS 引擎生成對應音頻，前端 AudioService 負責播放。若使用進階多模態模型（如 Google 的 **Gemini 2.0**），則可讓單一模型直接接受文字+圖像等輸入並產生文字、語音和圖像混合的輸出。整體上，各模態處理通過**服務化模組**進行封裝（語音識別服務、視覺分析服務、語音合成服務等），由後端的 Orchestrator 統籌調度，確保多模態信息流的時序協調。
    
    [github.com](https://github.com/eggyy1224/space_live_project/blob/main/README.md#:~:text=,%E7%AE%A1%E7%90%86%E7%8B%80%E6%85%8B%E5%92%8C%E6%A5%AD%E5%8B%99%E9%82%8F%E8%BC%AF%EF%BC%8C%E5%AF%A6%E7%8F%BE%E9%97%9C%E6%B3%A8%E9%BB%9E%E5%88%86%E9%9B%A2%E3%80%82)
    
    [blog.google](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#:~:text=multimodal%20inputs%20like%20images%2C%20video,defined%20functions)
    
- **架構設計建議:** 定義清晰的前後端**多模態通訊接口**：前端針對不同模態輸入，選擇合適的通道傳輸至後端。例如，音頻以 WebSocket 二進制流傳送以降低延遲，圖像則以壓縮JPEG通過 HTTP POST 上傳。後端在接收到語音文本、圖像分析結果後，將它們整合到對話上下文中（如將「攝影機識別結果：*觀眾微笑*」附加到LLM提示）。對話生成後，後端需要同時回傳多種輸出：文字回答、本地或外部生成的圖片（如需要展示星空照片）、以及 TTS 音頻或播放指令。前端 ChatInterface 收到文字後即刻顯示，同時 ModelViewer 根據動畫指令驅動虛擬人做出口型同步和表情變化，AudioControls 則播放語音。為確保不同模態同步，前端可以實現**狀態管理**（例如利用 Redux 或 Context 管理當前對話ID）：只有當文字、圖像、音頻三者都就緒或超時，一個回合才算完成，從而避免出現語音未播完就顯示下一句的情況。整合策略上，必要時引入**緩衝與隊列機制**，例如語音輸入時暫停角色主動發言，圖像分析未完成時稍微延遲 LLM 回應，以實現多模態協調的自然互動。
    
    [github.com](https://github.com/eggyy1224/space_live_project/blob/main/README.md#:~:text=,%E7%AE%A1%E7%90%86%E7%8B%80%E6%85%8B%E5%92%8C%E6%A5%AD%E5%8B%99%E9%82%8F%E8%BC%AF%EF%BC%8C%E5%AF%A6%E7%8F%BE%E9%97%9C%E6%B3%A8%E9%BB%9E%E5%88%86%E9%9B%A2%E3%80%82)
    
- **功能模組清單:**
    - **SpeechRecognition Service（語音識別服務）：** 接收前端音頻流並調用 STT 引擎，將語音轉寫為文字輸出給 DialogueManager。支持串流式轉寫，以便長段語音也能一邊輸入一邊獲取部分文字結果。
    - **VisionAnalysis Module（視覺分析模組）：** 分析前端傳來的圖像帧，執行如人臉識別、表情判定、物件與場景標識。將結果語義化（例如「偵測到2人微笑」）傳給對話系統作參考。
    - **Dialogue Orchestrator（對話編排器）：** 多模態對話核心，負責整合文字輸入（含STT結果）、視覺分析結果以及長期記憶內容，一併傳給 LLM 生成回應。並根據回應內容決定是否需要額外模態輸出（例如當回應提到「我給你看一張照片」時，調用圖像API獲取圖片）。
    - **SpeechSynthesis Service（語音合成服務）：** 調用 TTS 引擎將最終回覆文字轉為語音。支持多語種與情感語調合成，使虛擬主播的聲線符合角色個性（如活潑的語調）。生成後將音頻URL或緩衝直接發給前端播放。
    - **AvatarRender Controller（虛擬人渲染控制）：** 前端模組，負責根據後端指令更新3D角色的狀態。透過 React Three Fiber 操控 Three.js 角色模型的骨骼動畫和 Morph Targets 達成口型同步與表情切換。同時在場景中展示從後端接收的圖像（例如在虛擬場景的螢幕上播放一段影片或顯示一張圖片）。
        
        [github.com](https://github.com/eggyy1224/space_live_project/blob/main/README.md#:~:text=,%E7%AE%A1%E7%90%86%E7%8B%80%E6%85%8B%E5%92%8C%E6%A5%AD%E5%8B%99%E9%82%8F%E8%BC%AF%EF%BC%8C%E5%AF%A6%E7%8F%BE%E9%97%9C%E6%B3%A8%E9%BB%9E%E5%88%86%E9%9B%A2%E3%80%82)
        
- **可整合的外部 API:**
    - **語音服務 API：** 使用 *Google Cloud Speech-to-Text* 進行準確的語音轉寫，以及 *Google Cloud Text-to-Speech* 或 *Amazon Polly* 進行自然語音合成，以支援多語言和多種音色的主播聲音。
    - **計算機視覺 API：** 使用 *Google Cloud Vision API* 分析上傳的圖像，獲取物體、場景標籤或人臉表情等資料，作為 VisionAnalysis 模組的雲端替代方案。若需本地處理，可集成 OpenCV 或 TensorFlow.js 在瀏覽器端執行輕量圖像分析，減少伺服器負擔。
        
        [cloud.google.com](https://cloud.google.com/vision/docs/detecting-faces#:~:text=Face%20Detection%20detects%20multiple%20faces,emotional%20state%20or%20wearing%20headwear)
        
    - **圖像生成與資料庫 API：** 前端顯示圖片可來源於 *NASA API*（如天文每日圖，提供真實太空影像）或調用 *Stable Diffusion* 等生成模型創作插畫，使主播在講解太空概念時能動態展示相關圖片。
    - **多模態 AI 平台：** 隨技術成熟，可考慮使用 *Google Gemini 2.0* 的多模態API或 OpenAI 未來的多模態模型，一次性提交文字、圖像等輸入給模型，獲取綜合輸出（文字回答附帶語音和圖像），簡化多模態整合的複雜度。
        
        [blog.google](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#:~:text=multimodal%20inputs%20like%20images%2C%20video,defined%20functions)
        

## 4. 場景感知能力

- **技術實作方式與推薦框架:** 通過麥克風與攝影機實現**環境與觀眾狀態感知**。對音訊，可使用 **PyAudioAnalysis** 或 ML 模型分析環境聲音，區分語音與背景噪音，並檢測如笑聲、鼓掌等觀眾反應聲。對語音片段還可進一步進行情緒分析（如聲音激昂表示興奮）。對視覺，可利用 **MediaPipe** 或 **OpenCV** 在前端/後端進行即時人臉與人體偵測，獲取觀眾數量、位置和姿態。MediaPipe Face Mesh 可提供關鍵點以判斷觀眾是否專注（如視線方向）或情緒（笑容程度）。如需更高精度，可上傳影像至雲端 **Google Cloud Vision API** 執行人臉分析，直接得到情緒可能性（快樂、驚訝等）。這些感知技術結合後，可形成對場景的多維度理解，例如「當前有兩位觀眾，其中一位微笑一位打哈欠」。
    
    [cloud.google.com](https://cloud.google.com/vision/docs/detecting-faces#:~:text=Face%20Detection%20detects%20multiple%20faces,emotional%20state%20or%20wearing%20headwear)
    
- **架構設計建議:** 在前端啟用**感知資料收集**：允許取得攝影機畫面（經用戶授權）和環境音量。前端可定期擷取低頻次的影像幀（例如每秒1幀）發送後端，音訊則可抽取特徵（如平均能量、頻譜）上傳，以降低帶寬。後端設定**環境分析服務**，包含音訊分析器和影像分析器，各自獨立執行且低優先度（不阻塞主要對話）。分析結果更新在**全局場景狀態**中，例如：`scene_state = {人數:2, 情緒:["Neutral","Happy"], 噪音:"掌聲"}`。DialogueManager 或 BehaviorScheduler 每次生成動作前查閱 `scene_state`：若檢測到場景有特定事件則做出對應行為。比如 `scene_state` 顯示有人新加入（人數增加），則觸發主播插入一句歡迎；觀眾持續無反應且表情冷淡，則讓主播主動求問觀眾意見或改變話題。這種架構確保場景感知邏輯通過**狀態驅動**影響對話，而不需時刻中斷主流程。同時可將場景事件視作特殊的「輸入」餵給對話系統，使 AI 對這些事件有所描述或迴應，增強互動的臨場感。
- **功能模組清單:**
    - **AudioAnalyzer 模組：** 環境音分析器，持續監聽背景聲音並分類事件。如檢測笑聲時產生「audience_laugh」事件，檢測持續無聲則產生「silence」事件；對語音段落進行情緒分析以判斷說話者情緒（激動/平靜）。
    - **FaceDetector 模組：** 人臉偵測器，從攝影機畫面中辨識人臉位置和數量。標記出哪些人臉是新的（不在之前名單中）或消失的，並給每位觀眾分配ID以持續跟蹤。
    - **ExpressionAnalyzer 模組：** 表情分析器，基於人臉關鍵點或雲端服務結果判定觀眾情緒（如笑、愁、驚訝的概率）。也可檢測觀眾是否打哈欠、走神等狀態。
    - **AudienceStateTracker 模組：** 觀眾狀態追蹤，整合 FaceDetector 和 ExpressionAnalyzer 輸出，維護每個觀眾的狀態（如情緒隨時間的變化曲線）。提供高層摘要，如「總體情緒：熱烈」或「注意力下降」。
    - **ReactionPlanner 模組：** 反應規劃器，訂閱來自 AudioAnalyzer/AudienceState 的事件，根據預設規則決定主播行為。比如當 AudienceState 指示「注意力下降」且 IdleMonitor 也長時間空閒，則ReactionPlanner可通知 DialogueManager 插入一個引人注意的問題；當 AudioAnalyzer 發出「audience_laugh」時，讓角色跟著笑並詢問是否想到有趣的事情。
- **可整合的外部 API:**
    - **Google Cloud Vision 人臉分析：** 利用 Vision API 的人臉偵測獲取觀眾情緒屬性（如歡喜度高低），以及頭部姿態（低頭、仰頭）判斷注意力。這能豐富 ExpressionAnalyzer 的輸出。
        
        [cloud.google.com](https://cloud.google.com/vision/docs/detecting-faces#:~:text=Face%20Detection%20detects%20multiple%20faces,emotional%20state%20or%20wearing%20headwear)
        
    - **Azure Cognitive Services 視覺/語音：** Azure 提供人臉表情識別和語音情緒分析服務，可作為替代或補充。將即時視頻影像送至 Azure Face API 獲取笑容指數等；音頻片段送至 Azure Emotion Recognition 獲取聲音情緒。
    - **環境聲音分類 API：** 如 *Google Cloud Video Intelligence* 的音頻分類，能將背景聲分類為音樂、掌聲、講話等，幫助 AudioAnalyzer 更準確地識別環境事件（例如監測到掌聲時，讓主播反應「謝謝大家的鼓掌！」）。
    - **肢體動作識別框架：** 使用 *MediaPipe Holistic* 或 *OpenPose* 在視覺上捕捉觀眾的肢體語言（如揮手、點頭）。這些信號可通過外部工具檢測後傳入，豐富場景感知；例如偵測到觀眾揮手，主播可主動招手回應。
