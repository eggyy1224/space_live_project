。

**已具備能力：** 一般知識查詢、即時太空新聞、基礎天文資訊（如月相）這三方面的對應工具已經具備，並在對話中可用。專案前端也已實現對後端返回內容的即時呈現（包括3D模型表情、文字對話和語音播放）。例如，後端返回的文字消息會通過 WebSocket 推送到前端聊天框，若有對應的語音檔也會一起播放。前端的模型表情還會根據對話情緒即時變化，整體框架已能支撐較豐富的互動。

**尚待擴充：** 目前列出的其他工具類型，如**天氣地圖、深入占星解讀、多媒體天文圖像**等，尚未在現有 commit 中看到實裝跡象，屬於下一步計畫。語音方面，當前應用了 Google TTS 基礎能力，但**語者風格**尚未深度定制，未來可透過引入更多語音接口或參數調整來強化。此外，隨著工具增加，對話引擎的**工具管理**需持續優化，例如擴充`parse_tool_parameters`對更多參數類型的支援，以及`format_tool_result`針對圖片/音訊等特殊內容的處理。後端在回傳消息時，可能也需制定**統一的資料格式**來包含文字、圖像URL、音頻等多模態元素，以便前端可靠解析。根據需求，前端的展示邏輯也要相應加強：例如偵測到多張圖片時，自動切換到畫廊模式；收到長文本時支援滾動顯示等。

值得強調的是，**後端訊息的圖文結構**對前端體驗至關重要。未來在整合更多多模態工具時，後端可以規範每則對話消息為一個 JSON 對象，其中包含`text`字段（AI回答的文字）和可選的`images`列表（一到數個圖片URL），甚至`audio`字段（語音檔案連結或ID）。前端收到消息後，檢查到有多張`images`，就以**輪播圖**形式展示；只有一張圖片則作為插圖貼在文字旁。音頻則由 AudioService 自動播放。目前架構已經透過 WebSocket 實現即時推送，擴充這種結構只需在前後端協議上達成一致。以展示 NASA 圖片為例：後端從API獲得3張圖片，則打包成 `images: [url1, url2, url3]` 隨回答文字一起發出，前端聊天室會生成一個帶有三張可滑動圖片的卡片。用戶可以左右滑動查看，或讓其自動輪播，與此同時，AI的講解文字會固定在旁邊或下方。透過這種方式，**“星際小可愛”**將真正實現動態的圖、文、聲並茂互動，讓使用者彷彿身臨其境與太空站裡的 AI 網紅交流

[developers.google.com](https://developers.google.com/maps/documentation/maps-static/start#:~:text=The%20Maps%20Static%20API%20returns,your%20markers%20using%20alphanumeric%20characters)

[wilsjame.github.io](https://wilsjame.github.io/how-to-nasa/#:~:text=The%20APOD%20call%20returns%20a,received%20data%20however%20we%20want)

。

**總結**：整合上述各類 API 工具後，虛擬太空網紅將具備從天地氣象、星辰運勢、語音表達到宇宙影像講解的全方位互動能力。這些工具以結構化的資料（JSON、影像、音訊等）為 AI 提供了“感知”外部世界的管道，再結合 Gemini 2.0 Flash 模型強大的多模態生成和理解能力，最終經由 LangGraph 的流程編排將資訊無縫融入對話。在前後端通力配合下，用戶體驗到的將是一個有知識、有個性，能隨時分享**即時資訊與視覺內容**的太空站 AI 夥伴。各模組的持續完善和新工具的加入，會讓這個系統不斷進化，朝專案願景中所描繪的栩栩如生的虛擬網紅邁進。

[openweathermap.org](https://openweathermap.org/current#:~:text=Access%20current%20weather%20data%20for,JSON%2C%20XML%2C%20or%20HTML%20format)
