# TODO List: 整合情緒與說話狀態的臉部動畫 (04/10 更新 - 前端 Lipsync 完成)

**目標：** 實現一個整合系統，讓虛擬角色的表情能夠根據後端識別出的**多個情緒點**和語音時長，在前端動態生成平滑過渡的表情動畫，並結合**前端實時分析音頻生成**的基礎說話狀態（嘴型開合），達到**富有表現力和藝術性**的效果。

**核心思路：** 1. 定義數據格式 (完成) -> 2. 實現前端動畫邏輯 (完成) -> 3. 用模擬數據測試前端效果 (完成) -> 4. 修改後端產生並發送新數據 (完成) -> 5. 整合測試 (完成) -> 6. 問題排查與修復 (完成) -> 7. **實現前端 Lipsync (完成)** -> 8. **效果優化與完善 (進行中)**

**核心架構參考：** [speakemotion_todo.md](./speakemotion_todo.md) 中定義的四層架構（概念層面），但具體實現側重於前端插值。

**基於現有系統分析的整合步驟（新順序）：**

---

## 1. API 設計 (數據格式定義) - [完成]

*   [x] **確定並記錄 `emotionalTrajectory` 數據格式:** (包含 `type`, `payload` { `duration`, `keyframes` { `tag`, `proportion` } })
*   [x] **定義預設情緒標籤:** (已定義一套標準標籤列表)

## 2. 前端基礎準備 - [完成]

*   [x] **修改 `AudioService.ts` 或相關播放邏輯:** (已移除舊邏輯，添加 `isSpeaking` 狀態和 `audioStartTime` 記錄)
*   [x] **定義情緒基礎定義 (`Emotion Base Definitions`):** (已在 `emotionMappings.ts` 中定義基礎權重)

## 3. 實現前端核心動畫邏輯 - [完成]

*   [x] **定義中心計算點 (實現多情緒插值):** (`useEmotionalSpeaking` Hook 已實現插值邏輯)
*   [x] **實現說話狀態管理 (`Speech State Management Layer`):** (`useEmotionalSpeaking` Hook 已整合 `isSpeaking` 狀態)
*   [x] **實現 Blendshape 合併與混合策略 (`Blendshape Mixing Layer`):** (`useEmotionalSpeaking` Hook 中的 `calculateFinalWeights` 已實現初步混合邏輯)

## 4. 整合到渲染流程 - [完成]

*   [x] **更新 Zustand / R3F 狀態:** (`Model.tsx` 的 `useFrame` 已整合 `useEmotionalSpeaking` 的輸出和手動控制，並使用 Lerp 平滑)
*   [x] **解決權重覆蓋問題:** (`ModelService.ts` 和 `WebSocketService.ts` 已修改，確保情緒軌跡優先)
*   [x] **解決 Hook 調用錯誤:** (已修改 `Model.tsx` 使用 `useRef` 傳遞狀態到 `useFrame`)
*   [x] **實現權重合併:** (已在 `Model.tsx` 的 `useFrame` 中合併情緒和口型權重)
*   [ ] **移除/禁用 Fallback 動畫:** (此項待確認，可能舊的 fallback 嘴型動畫還在 `Model.tsx` 中)

## 5. 前端測試 (使用模擬數據) - [完成]

*   [x] **實現數據模擬方式:** (已在 `SettingsPanel.tsx` 添加模擬按鈕)
*   [x] **基礎功能測試:** (已驗證)
*   [x] **混合效果測試:** (已驗證)
*   [ ] **平滑度調優:** (可在整合測試後進行)
*   [ ] **視覺效果微調:** (可在整合測試後進行)

*總結：前端核心邏輯已開發完成，並通過模擬數據驗證可行。*

---

## 6. 後端實作 (文本生成 + 後續分析) - [完成]

*   [x] **(可選) 微調主 LLM Prompt (`prompting.py` & 提示模板):**
*   [x] **保持主 LLM 調用 (`llm_interaction.py` - `call_llm_node`):**
*   [x] **新增 Keyframe 分析節點/函數 (`dialogue_graph.py`):**
*   [x] **修改/確認 TTS 服務:**
*   [x] **修改 WebSocket 推送 (`websocket.py`):**
*   [x] **棄用舊 `AnimationService` 邏輯:**

## 7. 整合測試 - [完成]

*   [x] 啟動修改後的後端和前端。
*   [x] 通過聊天觸發後端，觀察 WebSocket 消息是否正確接收 `emotionalTrajectory`。
*   [x] 觀察前端模型是否按預期響應情緒軌跡和唇型同步。

## 8. 問題排查與修復 - [完成]

*   [x] **情緒軌跡動畫被覆蓋**：已解決。
*   [x] **渲染崩潰 (Invalid hook call)**：已通過在 `Model.tsx` 中使用 `useRef` 解決。
*   [x] **嘴形動畫 (Lipsync) 消失**：已通過修改 `ModelService.ts` 中 `handleLipsyncUpdate` 的 key 檢查邏輯解決。

## 9. 前端 Lipsync 實現 (基於音頻分析) - [完成]

*   [x] **在 `AudioService.ts` 中實現 Web Audio API 分析**: 創建 AnalyserNode，連接音頻源。
*   [x] **實時音量分析**: 使用 `requestAnimationFrame` 和 `getByteTimeDomainData` 計算 RMS 值。
*   [x] **映射到 `jawOpen`**: 根據 RMS 值更新 `jawOpen` Morph Target。
*   [x] **集成到播放流程**: 在音頻播放開始/結束時啟動/停止分析。
*   [x] **清理舊的 lipsync 處理代碼**: 移除 `ModelService.ts` 中的 `handleLipsyncUpdate`。

## 10. 效果優化與完善 - [進行中]

*   [ ] **完善情緒與口型混合策略 (Refinement)**:
    *   **當前狀態:** 情緒和音頻驅動的口型能同時工作，但口型（目前主要是 `jawOpen`）可能不夠精細，混合效果有待提升。
    *   **下一步:**
        *   調整 `AudioService.ts` 中 RMS 到 `jawOpen` 的映射參數 (`sensitivity`, `threshold`) 以獲得更自然的開合幅度。
        *   考慮分析更多音頻特徵（如頻率）來驅動其他口型 Morph Target（如 `mouthFunnel`, `mouthPucker`）。
        *   檢視 `useEmotionalSpeaking.ts` 和 `Model.tsx` 中的混合邏輯，確保情緒不過度影響說話時的口型。
*   [ ] **平滑度與視覺效果調優**: 細調 `useFrame` 中的 Lerp 因子，或調整 `emotionMappings.ts` 中的基礎權重，使動畫過渡更平滑、表情更生動。
*   [ ] **移除/禁用 Fallback 動畫 (Step 4.5)**: 確認新邏輯穩定後，清理舊的嘴型動畫。
*   [ ] **代碼清理**: 移除不再需要的日誌、註釋和舊代碼。

---

## 下一步建議 (更新後)

1.  **代碼清理 (Step 10.4)**: 移除不再需要的日誌、註釋和舊的/後備的動畫邏輯。
2.  **完善情緒與口型混合策略 (Step 10.1)**。
3.  **視覺效果與平滑度調優 (Step 10.2)**。
4.  **移除/禁用 Fallback 動畫 (Step 10.3)**。


**(舊的 TODO 內容已歸檔)** 