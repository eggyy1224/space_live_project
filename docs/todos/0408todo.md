# TODO List: 整合情緒與說話狀態的臉部動畫 (04/10 更新 - 後端實作方案調整)

**目標：** 實現一個整合系統，讓虛擬角色的表情能夠根據後端識別出的**多個情緒點**和語音時長，在前端動態生成平滑過渡的表情動畫，並結合基礎說話狀態（嘴型開合），達到**富有表現力和藝術性**的效果。

**核心思路：** 1. 定義數據格式 (完成) -> 2. 實現前端動畫邏輯 (完成) -> 3. 用模擬數據測試前端效果 (完成) -> 4. **修改後端以產生並發送新數據 (進行中 - 新方案)** -> 5. 整合測試。

**核心架構參考：** [speakemotion_todo.md](./speakemotion_todo.md) 中定義的四層架構（概念層面），但具體實現側重於前端插值。

**基於現有系統分析的整合步驟（新順序）：**

---

## 1. API 設計 (數據格式定義) - [完成]

*   [x] **確定並記錄 `emotionalTrajectory` 數據格式:** (包含 `type`, `payload` { `duration`, `keyframes` { `tag`, `proportion` } })
*   [x] **定義預設情緒標籤:** (已定義一套標準標籤列表)

## 2. 前端基礎準備 - [完成]

*   [x] **修改 `AudioService.ts` 或相關播放邏輯:** (已移除舊邏輯，添加 `isSpeaking` 狀態和 `audioStartTime` 記錄)
*   [x] **定義情緒基礎定義 (`Emotion Base Definitions`):** (已在 `emotionMappings.ts` 中定義基礎權重)

## 3. 實現前端核心動畫邏輯 - [完成]

*   [x] **定義中心計算點 (實現多情緒插值):** (`useEmotionalSpeaking` Hook 已實現插值邏輯)
*   [x] **實現說話狀態管理 (`Speech State Management Layer`):** (`useEmotionalSpeaking` Hook 已整合 `isSpeaking` 狀態)
*   [x] **實現 Blendshape 合併與混合策略 (`Blendshape Mixing Layer`):** (`useEmotionalSpeaking` Hook 中的 `calculateFinalWeights` 已實現初步混合邏輯)

## 4. 整合到渲染流程 - [完成]

*   [x] **更新 Zustand / R3F 狀態:** (`Model.tsx` 的 `useFrame` 已整合 `useEmotionalSpeaking` 的輸出和手動控制，並使用 Lerp 平滑)
*   [ ] **移除/禁用 Fallback 動畫:** (此項待確認，可能舊的 fallback 嘴型動畫還在 `Model.tsx` 中，需要在後端串接後移除)

## 5. 前端測試 (使用模擬數據) - [完成]

*   [x] **實現數據模擬方式:** (已在 `SettingsPanel.tsx` 添加模擬按鈕)
*   [x] **基礎功能測試:** (已驗證)
*   [x] **混合效果測試:** (已驗證)
*   [ ] **平滑度調優:** (可在整合測試後進行)
*   [ ] **視覺效果微調:** (可在整合測試後進行)

*總結：前端核心邏輯已開發完成，並通過模擬數據驗證可行。*

---

## 6. 後端實作 (進行中 - 新方案：文本生成 + 後續分析)

*目標：修改後端，先生成回應文本，然後獨立分析文本以生成 `keyframes`，最終組合 `duration` 發送 `emotionalTrajectory` 消息。*

*   [ ] **(可選) 微調主 LLM Prompt (`prompting.py` & 提示模板):**
    *   鼓勵 LLM 在回應文本中自然流露情感，但**不**強制要求輸出結構化 keyframes。
*   [ ] **保持主 LLM 調用 (`llm_interaction.py` - `call_llm_node`):**
    *   讓其繼續專注於生成高質量的**純文本**回應 (`llm_response_raw`)。
*   [ ] **新增 Keyframe 分析節點/函數 (`dialogue_graph.py`):**
    *   在 `DialogueGraph` 中，`call_llm_node` 之後添加一個新的處理步驟 (節點或直接調用)。
    *   **輸入:** `llm_response_raw` (純文本)。
    *   **實現:**
        *   **推薦:** 使用**第二次 LLM 調用** (可使用 Gemini) 專門分析文本並生成 keyframes。
        *   **定義 Keyframes JSON Schema:** 為這個**第二次**調用定義 JSON Schema，確保輸出格式正確 (`emotional_keyframes` 列表)。
        *   **設計分析 Prompt:** 編寫一個 Prompt，指示 LLM 分析輸入的文本，並根據情感生成符合 Schema 的 `emotional_keyframes`。
        *   **啟用 JSON 模式:** 在**第二次** LLM 調用時使用 JSON 模式。
        *   **實現驗證與回退:** 對第二次 LLM 調用返回的 `keyframes` 進行健壯的驗證（格式、標籤、範圍、排序、首尾幀）和回退處理 (如返回 `neutral` 軌跡)。
    *   **輸出:** 驗證後的 `emotional_keyframes`。
    *   在 `DialogueState` 中添加 `emotional_keyframes: Optional[List[Dict]] = None` 字段並存儲結果。
*   [ ] **修改/確認 TTS 服務:**
    *   確保 `TextToSpeechService` 返回準確的語音時長 `duration` (秒)。
    *   在 `DialogueGraph` 或相關流程中獲取此 `duration`。
*   [ ] **修改 WebSocket 推送 (`websocket.py`):**
    *   從 `DialogueState` 或處理結果中獲取 `duration` 和 `emotional_keyframes`。
    *   構建並**發送 `emotionalTrajectory` 消息**。
    *   **移除/註釋掉** 發送舊的 `morph_update` 或 `lipsync_update` 消息的程式碼。
*   [ ] **棄用舊 `AnimationService` 邏輯:**
    *   移除或註釋掉 `AnimationService` 中 `create_lipsync_morph` 等不再需要的方法及其調用。

## 7. (Bonus/Future) 整合更高級的輸入與功能

*   [ ] **實現精確 Lip-sync:**
*   [ ] **添加隨機/程序化動畫:**

---

## 筆記區 (API 設計與情緒映射思考)

**(保留不變，但 Keyframe 生成方式已改為二次分析)**

---

## 下一步建議 (更新後)

1.  **完成後端實作 (Step 6 - 新方案)**:
    *   在 `DialogueState` 添加 `emotional_keyframes` 字段。
    *   實現 `analyze_text_for_keyframes` 邏輯（包含二次 LLM 調用 + JSON 模式 + 驗證）。
    *   在 `DialogueGraph` 中整合此新步驟。
    *   確認 TTS 返回 `duration`。
    *   修改 WebSocket 推送以發送 `emotionalTrajectory` 並移除舊消息。
    *   棄用舊 `AnimationService` 邏輯。
2.  **整合測試 (使用實際後端數據)**:
    *   啟動修改後的後端和前端。
    *   通過聊天觸發後端，觀察 WebSocket 消息是否正確接收 `emotionalTrajectory`。
    *   觀察前端模型是否按預期響應。
3.  **完善情緒混合策略 (Refinement)**:
    *   根據整合測試效果，微調 `useEmotionalSpeaking.ts` 中的混合邏輯。
4.  **驗證其他核心功能**: 確認聊天、攝影機控制等。
5.  **移除/禁用 Fallback 動畫 (Step 4.2)**: 確認新邏輯穩定後，清理舊的嘴型動畫。
6.  **平滑度與視覺效果調優 (Step 5.4, 5.5)**: 細調 Lerp 和基礎權重。
7.  **刪除 Legacy Code**: 清理後端不再使用的舊動畫服務等程式碼。


# 功能重建與測試階段 TODO (04/09 更新)

## 主要目標：恢復核心功能穩定性並驗證新情緒邏輯

1.  **識別損壞的功能**:
    *   **[已解決]** 控制面板無法控制表情 (Morph Targets)。
    *   (請在此處補充其他損壞功能 - *待確認*)
    *   檢查瀏覽器開發者工具 (Console, Network) 和 React/Redux DevTools 中的錯誤訊息。

2.  **診斷與修復**:
    *   **(針對控制面板)** 分析並修復 `Model.tsx` 的 `useFrame` 邏輯以融合手動/預設權重與自動權重。**(已完成)**
    *   Store/State Management: 基本檢查完成，狀態流動正常。
    *   WebSocket 連接與訊息處理: 基本連接正常，模擬消息接收正常。
    *   模型渲染與動畫: 基本渲染和動畫正常。
    *   元件互動: 控制面板與模型互動已修復。
    *   (根據損壞功能列表具體分析)

3.  **核心功能驗證**:
    *   模型加載與顯示: **[已驗證]**
    *   基本動畫播放: **[已驗證]**
    *   WebSocket 連接: **[已驗證]**
    *   聊天功能: (*待確認*)
    *   其他核心交互（攝影機控制等）: (*待確認*)
    *   控制面板手動表情: **[已驗證]**

4.  **前端情緒化說話邏輯測試 (使用模擬數據)**:
    *   在 `SettingsPanel.tsx` 添加測試按鈕以模擬 `emotionalTrajectory` 數據。**(已完成)**
    *   添加「同時講話與情緒變化」的模擬按鈕。**(已完成)**
    *   使用模擬數據驗證 `useEmotionalSpeaking` 的計算、混合邏輯和 `Model.tsx` 中的權重應用。**(已驗證)**

---

## 下一步建議

*   **整合測試 (使用實際後端數據)**:
    *   確保後端能夠正確生成並通過 WebSocket 發送 `emotionalTrajectory` 訊息。
    *   觸發後端（例如通過聊天）生成包含情緒的回應，觀察前端模型是否按照實際數據正確響應。
*   **完善情緒混合策略 (Refinement)**:
    *   檢視 `useEmotionalSpeaking.ts` 中 `calculateFinalWeights` 的 `// TODO: 實現混合策略` 部分。
    *   根據實際測試效果（無論是模擬還是真實數據），微調 `jawOpen`, `mouthClose` 及其他相關 Blendshape 的混合方式，使表情更自然。
*   **驗證其他核心功能**: 確認聊天、攝影機控制等其他核心功能是否都正常運作。
*   **刪除 Legacy Code**: 在確認所有功能穩定後，清理不再使用的舊程式碼。

---

## 暫緩的任務

*   (Legacy code removal 現在是可能的下一步，取決於功能穩定性) 