# TODO List: 整合情緒與說話狀態的臉部動畫 (0408版 v3 - 前端優先，驗證後調整後端)

**目標：** 實現一個整合系統，讓虛擬角色的表情能夠根據後端識別出的**多個情緒點**和語音時長，在前端動態生成平滑過渡的表情動畫，並結合基礎說話狀態（嘴型開合），達到**富有表現力和藝術性**的效果。

**核心思路：** 1. 定義數據格式 -> 2. 實現前端動畫邏輯 -> 3. 用模擬數據測試前端效果 -> 4. 修改後端以產生數據。

**核心架構參考：** [speakemotion_todo.md](./speakemotion_todo.md) 中定義的四層架構（概念層面），但具體實現側重於前端插值。

**基於現有系統分析的整合步驟（新順序）：**

---

## 1. API 設計 (數據格式定義)

*   [ ] **確定並記錄 `emotionalTrajectory` 數據格式:**
    *   明確前端期望從後端 WebSocket 收到的、用於驅動情緒動畫的數據結構。
    *   **最終格式建議：**
      ```json
      {
        "type": "emotionalTrajectory", // WebSocket 消息類型
        "payload": {
          "duration": number, // 總語音秒數 (由 TTS 或其他方式提供)
          "keyframes": [ 
            // 情緒關鍵幀列表，按時間比例排序
            { "tag": string, "proportion": number } // 情緒標籤 + 時間比例 (0.0 到 1.0)
          ]
        }
      }
      ```
    *   例子: `keyframes: [{ "tag": "neutral", "proportion": 0.0 }, { "tag": "happy", "proportion": 0.2 }, { "tag": "surprised", "proportion": 0.6 }, { "tag": "happy", "proportion": 1.0 }]`
*   [ ] **定義預設情緒標籤:**
    *   確定一套前後端共用的情緒標籤字符串 (e.g., "neutral", "happy", "sad", "angry", "surprised", "thinking", "listening"...)。

## 2. 前端基礎準備

*   [x] **修改 `AudioService.ts` 或相關播放邏輯:**
    *   移除在 `updateMouthShape` 中直接調用 `useStore.getState().updateMorphTarget('jawOpen', ...)` 的邏輯。
    *   在 `startRecording`/`stopRecording` 或 `playAudio`/`stopPlayback` 時，更新一個 Zustand 狀態來標記 `isSpeaking` (True/False)。
    *   在 `playAudio` 開始播放時，需要記錄**開始播放的時間戳**，或觸發一個事件通知「中心計算點」語音已開始，以便計算 `elapsedTime`。
    *   (可選) 可以將錄音時的 `normalizedAverage` (音量) 儲存到狀態供未來使用。
*   [x] **定義情緒基礎定義 (`Emotion Base Definitions`):** (初步完成，待測試調優)
    *   在前端定義（或從配置加載）每個情緒標籤對應的基礎 Blendshape 目標權重 (`emotionBaseWeights: Record<string, Record<string, number>>`)。
    *   **允許誇張:** 在定義這些基礎權重時，可以大膽設置數值，以追求期望的藝術效果。
    *   可以參考 `speakemotion_todo.md` 中的 Blendshape 分類。

## 3. 實現前端核心動畫邏輯

*   [x] **定義中心計算點 (實現多情緒插值):** (Hook 結構與插值邏輯完成)
    *   實現動畫計算的核心邏輯，推薦使用**自定義 Hook (e.g., `useEmotionalSpeaking`)**。
    *   此 Hook 需要：
        *   從 Zustand 或 props 獲取 `isSpeaking` 狀態。
        *   能夠接收並存儲模擬的或來自 WebSocket 的 `emotionalTrajectory` 數據 (`duration` 和 `keyframes`)。
        *   獲取語音播放的**開始時間戳**。
        *   在每一幀 (`useFrame` 內部或由其觸發)：
            *   計算自語音開始以來的**已播放時間 `elapsedTime`**。
            *   計算當前進度 `progress = clamp(elapsedTime / duration, 0, 1)`。
            *   根據 `progress` 在 `keyframes` 之間查找包圍它的前後兩個關鍵幀 `kf_prev`, `kf_next`。
            *   計算在這兩個關鍵幀之間的局部插值比例 `localProgress`。
            *   獲取 `kf_prev.tag` 和 `kf_next.tag` 的基礎權重 (`emotionBaseWeights`)。
            *   **執行插值：** 計算當前幀的情緒目標權重 `currentEmotionWeights`。
*   [x] **實現說話狀態管理 (`Speech State Management Layer`):** (已整合入 Hook)
    *   實現一個簡單函數，根據 `isSpeaking` 狀態輸出基礎的說話 Blendshape 目標權重 (`speechBlendshapeWeights`)。
*   [x] **實現 Blendshape 合併與混合策略 (`Blendshape Mixing Layer`):** (已整合入 Hook)
    *   實現一個函數，接收計算出的 `currentEmotionWeights` 和 `speechBlendshapeWeights`。
    *   根據預定義的混合策略（例如 `jawOpen` 取最大值，其他直接合併或加權），計算最終的 `finalTargetWeights`。
    *   **輸出：** 將 `finalTargetWeights` 返回給調用者（可能是 `useFrame`）。

## 4. 整合到渲染流程

*   [ ] **更新 Zustand / R3F 狀態:**
    *   修改 `useFrame` (可能在 `Model.tsx` 或由 `useEmotionalSpeaking` Hook 返回更新函數供 `useFrame` 調用)。
    *   讓 `useFrame` 在每一幀從中心計算點獲取 `finalTargetWeights`。
    *   **[重要]** 確保 `useFrame` 中的 Lerp 邏輯**仍然存在**。中心計算點計算的是**目標值 (target)**，`useFrame` 中的 Lerp 負責將模型的**當前值 (current)** 平滑地趨近這個**目標值**。不要將中心計算點的輸出直接賦值給 `morphTargetInfluences`。
*   [ ] **移除/禁用 Fallback 動畫:**
    *   找到 `Model.tsx` 中 `useFrame` 裡的 fallback 嘴型動畫邏輯。
    *   將其禁用或移除。

## 5. 前端測試 (使用模擬數據)

*   [ ] **實現數據模擬方式:**
    *   **[關鍵步驟]** 在前端創建一種方式來**模擬接收後端的 `emotionalTrajectory` 消息**。
    *   方式可選：
        *   添加一個開發用的 UI 按鈕，點擊時觸發一段預設的 `emotionalTrajectory` 數據送入 `useEmotionalSpeaking` Hook。
        *   修改 WebSocket 接收邏輯，使其在開發模式下可以接收手動輸入或預設的測試數據。
        *   在 `useEmotionalSpeaking` Hook 內部硬編碼一些測試數據，通過狀態切換來觸發。
*   [ ] **基礎功能測試:**
    *   測試 `isSpeaking` 切換時，基礎嘴型開合是否正常。
    *   用模擬數據測試單一情緒關鍵幀時（例如 `keyframes: [{ tag: 'happy', proportion: 0.0 }, { tag: 'happy', proportion: 1.0 }]`），角色是否能穩定顯示該表情。
*   [ ] **混合效果測試:**
    *   **[核心驗證]** 使用包含**多個不同情緒關鍵幀**的模擬數據，仔細觀察表情是否在指定的 `duration` 內**平滑且自然地流動過渡**。
    *   驗證過渡效果是否符合你的**藝術性預期**。
    *   測試在情緒過渡過程中，`isSpeaking` 狀態切換的效果。
*   [ ] **平滑度調優:**
    *   調整 `Model.tsx` 中 `useFrame` 的 `lerpFactor`，控制視覺上的平滑速度。
    *   檢查前端插值計算的性能。
*   [ ] **視覺效果微調:**
    *   根據測試反饋，**重點調整**各情緒的基礎 Blendshape 權重 (`emotionBaseWeights`)，直到達到滿意的藝術風格和誇張度。
    *   調整說話時 `jawOpen` 的目標值、`mouthClose` 的值等。
    *   調整混合策略。

## 6. 後端調整 (在前端驗證後)

*   [ ] **增強情緒識別與定位:**
    *   **[挑戰點]** 修改 `EmotionAnalyzer` 或 `DialogueGraph` 中的邏輯，使其能夠分析一段對話文本，識別出其中包含的**多個主要情緒點**，並**估計這些情緒點在對應語音中的相對時間位置 (proportion)**。
*   [ ] **實現新的數據發送:**
    *   修改後端 WebSocket 推送邏輯，確保在 `DialogueGraph` 處理完成後，能獲取到計算出的 `duration` 和 `keyframes`，並按照**步驟 1 中定義的格式**發送 `emotionalTrajectory` 消息。
*   [ ] **獲取 `duration`:**
    *   確定 `duration` (總語音秒數) 的來源。最可能來自 `TextToSpeechService` 返回的結果。
*   [ ] **棄用舊的後端動畫計算:**
    *   移除 `AnimationService.create_lipsync_morph` 的調用和相關邏輯。
    *   確保不再發送舊的 `morph_update` 或 `lipsync_update` 消息。

## 7. (Bonus/Future) 整合更高級的輸入與功能

*   [ ] **實現精確 Lip-sync:**
    *   研究並實施基於 Viseme/Phoneme 的口型同步方案。
    *   將 Lip-sync 計算出的口型 Blendshape 目標權重，整合到 `Blendshape Mixing Layer` 中。
*   [ ] **添加隨機/程序化動畫:**
    *   重新引入或改進隨機眨眼 (`eyeBlinkLeft/Right`) 等。

---

## 筆記區 (API 設計與情緒映射思考)

**(此區內容可隨時更新)**

**1. `emotionalTrajectory` 數據格式確認:**

後端通過 WebSocket 發送，前端接收的數據結構：

```json
{
  "type": "emotionalTrajectory",
  "payload": {
    "duration": number,       // 語音總秒數 (由 TTS 服務提供)
    "keyframes": [
      { 
        "tag": string,       // 情緒標籤 (e.g., "happy", "sad")
        "proportion": number // 時間比例 (0.0 到 1.0)
      }
      // ... 可能有多個關鍵幀, 按 proportion 排序
    ]
  }
}
```

**2. 情緒標籤 (Tag) 列表:**

*   需要在 TODO 步驟 1.2 中定義一套標準的情緒標籤列表，供前後端共用。
*   初步建議標籤: `neutral`, `happy`, `sad`, `angry`, `surprised`, `fearful`, `disgusted`, `thinking`, `listening`
*   *待討論：是否需要更多細微或複合情緒標籤？ `speaking_default` 似乎可以由前端混合邏輯處理，不一定需要獨立標籤。*

**3. 情緒標籤到 Blendshape 的映射 (`emotionBaseWeights`):**

*   **核心：** 這個映射關係**定義在前端** (參見 TODO 步驟 2.2)。
*   前端需要維護一個類似 `Record<string, Record<string, number>>` 的結構。
*   **參考 `speakemotion_todo.md` 的分類來定義基礎權重:** (示例見文件先前版本，前端需實現完整映射)
    *   **`happy`**: `mouthSmile*`, `cheekSquint*`, `eyeSquint*` ...
    *   **`sad`**: `browInnerUp`, `mouthFrown*`, `mouthStretch*` ...
    *   **`surprised`**: `eyeWide*`, `browOuterUp*`, `jawOpen` (微張)...
    *   **`angry`**: `browDown*`, `noseSneer*`, `mouthPress*`, `jawForward` ...
    *   **`neutral`**: 所有情緒相關 Blendshape 值為 0。
*   **藝術性控制:** 前端定義權重時可自由發揮，允許誇張。
*   **完整性:** 前端映射需覆蓋所有定義的 `tag`。

**4. `emotionalTrajectory` 的生成方式 (設計決策):**

*   **採用 LLM 直接生成:**
    *   **機制:** 在後端調用 LLM 生成對話文本時，通過 Prompt 指示 LLM 同時分析自身回應的情感流動，並直接輸出結構化的 `keyframes` (包含 `tag` 和近似的 `proportion`)。
    *   **後端處理:**
        *   修改 `DialogueGraph` 中的 LLM 調用節點，更新 Prompt 以包含生成 `keyframes` 的指令。
        *   從 LLM 響應中解析出文本和 `keyframes` 數據。
        *   **[關鍵]** 實現**健壯的驗證與回退邏輯**：
            *   驗證 `keyframes` 數據格式是否正確。
            *   檢查 `tag` 是否在預定義列表中。
            *   檢查 `proportion` 是否介於 0.0-1.0 且大致有序 (允許微小誤差，後端可排序修正)。
            *   確保第一個 `proportion` 為 0.0，最後一個為 1.0 (如果 LLM 未提供，後端可補充 `neutral` 或其他默認狀態的頭尾幀)。
            *   **回退機制:** 如果 LLM 輸出格式錯誤或無法解析，後端應發送一個安全的默認值，例如只包含 `neutral` 的關鍵幀：`keyframes: [{ "tag": "neutral", "proportion": 0.0 }, { "tag": "neutral", "proportion": 1.0 }]`。
        *   從 TTS 服務獲取準確的 `duration`。
        *   組合驗證/修正後的 `keyframes` 和 `duration`，發送 `emotionalTrajectory` 消息。
    *   **優點:** 情緒與文本內在一致性高；可能簡化後端服務調度；滿足近似時間要求。
    *   **挑戰:** LLM 輸出穩定性；Prompt 設計；潛在延遲增加；文本比例與語音時長的映射可能不完美 (但對藝術效果可能足夠)。

**5. 前端插值邏輯:**

*   前端的 `useEmotionalSpeaking` Hook 根據收到的 `keyframes` 和當前的 `elapsedTime`，在相鄰關鍵幀的 `emotionBaseWeights` 之間進行線性插值，計算 `currentEmotionWeights`。

**6. 待討論/思考:**

*   ~~後端識別情緒關鍵幀的時間比例 (`proportion`) 的精度需要多高？~~ -> 決定使用 LLM 估計，接受其近似性。
*   是否需要強度 (`intensity`) 信息？ -> 初步設計保持簡單，暫不加入。後續可考慮讓 LLM 同時輸出強度。
*   `neutral` 狀態的處理：由後端驗證邏輯確保首尾幀存在 (如果 LLM 未提供)。
*   前端插值方法：目前預設線性插值 (Lerp)，未來是否需要更平滑的插值方法 (如 Ease in/out, Spline)？ (初步保持 Lerp 簡單)

這份 TODO 列表旨在提供一個清晰的路徑圖，將你的設計方案逐步整合到現有程式碼中。祝你實作順利！

# 功能重建階段 TODO (04/09 更新)

## 主要目標：恢復核心功能穩定性

1.  **識別損壞的功能**:
    *   **[已解決]** 控制面板無法控制表情 (Morph Targets)。
    *   (請在此處補充其他損壞功能)
    *   檢查瀏覽器開發者工具 (Console, Network) 和 React/Redux DevTools 中的錯誤訊息。

2.  **診斷與修復**:
    *   **(針對控制面板)** 分析控制面板組件、`ModelViewer.tsx`、`Model.tsx` 以及 Zustand store 之間的交互，找出導致手動控制失效的原因。**(已修改 Model.tsx 的 useFrame 邏輯以融合權重)**
    *   Store/State Management: 檢查 Zustand store 的狀態是否按預期更新。
    *   WebSocket 連接與訊息處理: 確認連接和訊息流。
    *   模型渲染與動畫: 檢查 `ModelViewer.tsx` 和 `Model.tsx`。
    *   元件互動: 審查組件間的互動。
    *   (根據損壞功能列表具體分析)

3.  **驗證核心功能**:
    *   確保模型能夠成功加載並顯示。
    *   確保基本的閒置動畫 (Idle) 或預設動畫可以播放。
    *   確保 WebSocket 能夠連接。
    *   確保聊天功能（如果存在）基本可用。
    *   確保之前存在的其他核心交互（如攝影機控制）正常。
    *   **[已驗證]** 確保控制面板可以手動設置預設表情，並能在重置後恢復自動表情。

---

## 暫緩的任務 (待核心功能穩定後)

*   **完善情緒混合策略**: 檢視 `useEmotionalSpeaking.ts` 中 `calculateFinalWeights` 的 TODO，根據視覺效果調整混合方式。
*   **刪除 Legacy Code**: 清理不再使用的舊程式碼。 