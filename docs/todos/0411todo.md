# 2024-04-11 TODO

## 效能調校 - 提升回覆速度

### 後端優化

- [ ] 分析主要延遲來源：LLM 推理 (DialogueGraph 中的 call_llm_node 和 analyze_keyframes_node) 與 TTS 語音合成。
- [ ] 研究合併 DialogueGraph 中的兩次 LLM 呼叫，評估使用 Function Calling 或單次 JSON 輸出的可行性與效能。
- [ ] 評估並優化 TTS 服務 (text_to_speech.py)：研究更快的模型/設定，或改為非同步生成 (先回文字，後推語音)。
- [ ] 分析 MemorySystem (memory_system.py) 中記憶檢索 (特別是 ChromaDB 查詢) 的效能。
- [ ] 檢視 websocket.py 中的非同步流程，確保 AI、TTS、檔案儲存等操作盡可能併發執行，減少阻塞。
- [ ] 監控並評估 DialogueGraph 中外部工具 (維基百科、新聞 API 等) 呼叫的延遲影響。

### 前端優化

- [x] **改善視覺反饋** (無需修改 API 結構，提升感知速度)：
    - [x] 打字機效果：實作文字逐字顯示功能，讓用戶在等待完整回覆時能看到文字「生成中」效果。
    - [x] 生動的等待動畫：為角色添加「思考中」的泡泡框動畫狀態，使等待過程更生動有趣 (已修復 `isProcessing` 狀態管理)。
    - [x] 進度指示器：添加處理階段指示，清晰顯示「正在思考」→「正在組織語言」→「正在生成語音」等階段。
- [x] **同步語音與文字顯示**：
    - [x] 將打字機效果的速度與語音長度同步，提供更自然的閱讀體驗。
    - [x] 優化思考泡泡框設計，使等待過程更直觀。
- [x] **修正前端錯誤與細節**：
    - [x] 修正使用者發送的訊息重複顯示的問題。
    - [x] 修正打字機游標位置，使其緊跟文字。
    - [x] 為打字機游標添加閃爍效果。
    - [x] 為正在輸入的訊息泡泡添加光暈效果。

### 效能測試與監控

- [ ] 研究並導入合適的回覆速度測試工具 (例如日誌分析或專門工具)。
- [x] **建立基準測量**: 
    - [x] 在前端 (如 ChatService.ts) 實作使用者端到端延遲測量：從發送訊息到接收到第一個 `chat-message` 回覆的時間。
    - [x] 在後端 (websocket.py) 為 `chat-message` 處理流程添加詳細計時日誌：記錄接收請求、AI 呼叫起訖、TTS 呼叫起訖、音檔儲存(若有)、發送回覆等關鍵時間點。
- [ ] (進階) 在後端關鍵元件 (DialogueGraph 節點、MemorySystem 查詢、TTS API 呼叫) 內部添加更細粒度的耗時監控日誌。
- [ ] 基準測量結果分析：當前總體延遲約 5.8 秒，包括 AI 服務 (57.5%, 約 3.3 秒) 和 TTS 服務 (42.5%, 約 2.5 秒)。

### 後端優化

- [ ] **階段一：非同步處理架構改造** (預計可減少 42.5% 的感知延遲)
    - [ ] 修改 websocket.py 中的處理流程，分離文字回覆與 TTS 處理：
        - [ ] AI 生成回覆後，立即發送包含文字內容的 chat-message
        - [ ] 同時在背景開始非同步 TTS 處理
        - [ ] TTS 完成後，發送包含語音 URL 的更新訊息
    - [ ] 調整前端 ChatService 的消息處理邏輯，支持接收語音更新
    - [ ] 在 websocket.py 中確保背景 TTS 任務的異常處理，避免記憶體洩漏

- [ ] **階段二：AI 服務優化** (預計可減少 10-30% 的 AI 處理時間)
    - [x] 在 DialogueGraph 中添加細粒度計時，識別最耗時的子流程。
        - [x] 為 DialogueGraph 中各節點添加計時日誌
        - [ ] 特別監控 LLM 呼叫、記憶檢索、工具調用等環節。
    - [ ] **優化 DialogueGraph 執行流程**:
        - [x] **調查並優化 `detect_tool_intent` 節點** (之前耗時異常)
            - [x] **實施方案**: 將該節點及其依賴的 `parse_tool_parameters` 節點內的 LLM 呼叫，**切換為使用更快的 `gemini-1.5-flash-8b` 模型**，以平衡速度與智能。
        - [ ] **非同步記憶儲存**：將記憶儲存 (`store_memory_node`) 移至回應發送後，在背景異步執行，減少主流程等待時間。
        - [ ] 研究合併 `call_llm_node` 和 `analyze_keyframes_node` 的可行性。
        - [ ] 評估 LLM 參數調整 (如降低 `max_tokens`, `temperature`) 對速度的影響。
    - [ ] 優化記憶系統：
        - [ ] 分析並優化 ChromaDB 查詢效能。
        - [ ] 評估記憶檢索策略，避免檢索過多不相關記憶。
- [ ] **階段三：TTS 服務優化**
    - [ ] **研究流式 TTS (Streaming TTS)**：評估是否可以讓 TTS 在接收到部分文字後就開始生成音訊，以更快地開始播放。
    - [ ] **研究並行處理**: 評估能否讓 TTS 生成與 AI 後續處理或網路傳輸並行執行。
    - [ ] 研究更快的 TTS 模型或設定。

### 後端優化

- [ ] **階段一：非同步處理架構改造** (預計可減少 42.5% 的感知延遲)
    - [ ] 修改 websocket.py 中的處理流程，分離文字回覆與 TTS 處理：
        - [ ] AI 生成回覆後，立即發送包含文字內容的 chat-message
        - [ ] 同時在背景開始非同步 TTS 處理
        - [ ] TTS 完成後，發送包含語音 URL 的更新訊息
    - [ ] 調整前端 ChatService 的消息處理邏輯，支持接收語音更新
    - [ ] 在 websocket.py 中確保背景 TTS 任務的異常處理，避免記憶體洩漏

- [ ] **階段二：AI 服務優化** (預計可減少 10-30% 的 AI 處理時間)
    - [x] 在 DialogueGraph 中添加細粒度計時，識別最耗時的子流程。
        - [x] 為 DialogueGraph 中各節點添加計時日誌
        - [ ] 特別監控 LLM 呼叫、記憶檢索、工具調用等環節。
    - [ ] **優化 DialogueGraph 執行流程**:
        - [x] **調查並優化 `detect_tool_intent` 節點** (之前耗時異常)
            - [x] **實施方案**: 將該節點及其依賴的 `parse_tool_parameters` 節點內的 LLM 呼叫，**切換為使用更快的 `gemini-1.5-flash-8b` 模型**，以平衡速度與智能。
        - [ ] **非同步記憶儲存**：將記憶儲存 (`store_memory_node`) 移至回應發送後，在背景異步執行，減少主流程等待時間。
        - [ ] 研究合併 `call_llm_node` 和 `analyze_keyframes_node` 的可行性。
        - [ ] 評估 LLM 參數調整 (如降低 `max_tokens`, `temperature`) 對速度的影響。
    - [ ] 優化記憶系統：
        - [ ] 分析並優化 ChromaDB 查詢效能。
        - [ ] 評估記憶檢索策略，避免檢索過多不相關記憶。
- [ ] 